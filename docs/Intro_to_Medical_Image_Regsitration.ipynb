{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shared Intro_to_Medical_Image_Regsitration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6HaLO5rCf9",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Medical Image Registration using Deep Neural Networks with DeepReg\n",
        "\n",
        "### Authors: \n",
        "  - Nina Montana Brown <sup>1, 2</sup>\n",
        "  - Yunguan Fu <sup>1, 2, 3</sup>\n",
        "  - Shaheer Saeed <sup>1, 2</sup>\n",
        "  - Adrià Casamitjana <sup>2</sup>\n",
        "  - Zachary Baum <sup>1, 2</sup>\n",
        "  - Rémi Delaunay <sup>1, 2, 4</sup>\n",
        "  - Qianye Yang <sup>1, 2</sup>\n",
        "  - Alexander Grimwood <sup>1, 2</sup>\n",
        "  - Zhe Min <sup>1</sup>\n",
        "  - Ester Bonmati <sup>1, 2</sup>\n",
        "  - Tom Vercauteren <sup>4</sup>\n",
        "  - Matthew J. Clarkson <sup>1, 2</sup>\n",
        "  - Yipeng Hu <sup>1, 2</sup>\n",
        "\n",
        "Affiliations:\n",
        " - [1] Wellcome/EPSRC Centre for Surgical and Interventional Sciences, University College London\n",
        " - [2] Centre for Medical Image Computing, University College London\n",
        " - [3] InstaDeep\n",
        " - [4] Department of Surgical & Interventional Engineering, King’s College London\n",
        "\n",
        "# Table of Contents - Update\n",
        "1. [Objective of the tutorial](#obj)\n",
        "2. [Set-up](#setup)\n",
        "3. [Introduction to Registration](#IntroReg)\n",
        "4. [Registration with Deep Learning](#DeepRegistrationIntro)\n",
        "5. [Two classical registration Examples](#classical-examples) \n",
        "6. [Medical Image registration using an adapted DeepReg Demo](#deep-example)\n",
        "7. [References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7RjsIyi7CqZ",
        "colab_type": "text"
      },
      "source": [
        "# Objective <a name=\"obj\"></a>\n",
        "This tutorial introduces a new open-source project [DeepReg](https://github.com/DeepRegNet/DeepReg), which is designed to be a rookie-friendly package for researchers interested in image registration using deep learning. \n",
        "\n",
        "A previous MICCAI workshop [learn2reg](https://learn2reg.github.io/) provided an excelent example of novel algorithms and interesting approaches in this active research area, whilst this tutorial explores the strength of the simple, yet generalisable design of DeepReg. \n",
        "- Explain basic concepts in medical image registration;\n",
        "- Explore the links between the modern algorithms using neural networks and the classical iterative algorithms (which can also be readily implemented using DeepReg!);\n",
        "- Perhaps more importantly, introduce the versatile capability of DeepReg, with diverse examples in real clinical challenges.\n",
        "\n",
        "All of this will require minimum scripting and coding expereince with DeepReg, accompanying a set of well-written documentation and a growing number of demos using real, open-accesible clinical data. This tutorial will get you started with a number of examples by step-by-step instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey3CnrZuBtCi",
        "colab_type": "text"
      },
      "source": [
        "# Set-up <a name=\"setup\"></a>\n",
        "This tutorial depends on the package [DeepReg](https://github.com/DeepRegNet/DeepReg), which in turn has external dependencies which are managed by `pip`. The current version is implemented as a Tensorflow 2-based framework and relies on Python>=3.7.\n",
        "\n",
        "To ensure the demo'd algorithms can run from this Google Colab notebook, you will need to mount your Google Drive and clone the repository into it. You can do this by running the following commands.\n",
        "\n",
        "You can also follow along in a local copy of the repo using your own python scripts - in this case, follow instructions for set up at [the quickstart guide](https://github.com/DeepRegNet/DeepReg/blob/master/docs/quick_start.md).\n",
        "\n",
        "Training DNNs is computationally expensive. We have tested this demo with GPUs provided by Google through Google Colab. Training times have been roughly measured and indicated where appropriate. You can run this on CPU but we have not tested how long it would take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBep99T7DMvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4a521392-534a-4092-90a0-9c160cbbef75"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLZnPHlqZM9R",
        "colab_type": "text"
      },
      "source": [
        "Once the drive is mounted, ensure that you have GPU enabled for more efficient training.\n",
        "\n",
        "To do this, go to the Edit tab on the upper left hand bar: Edit > Click on Notebook Settings > Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oedTHyNoDWzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a084fa5-f13e-447d-b48d-314d46b854fd"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(r'/content')\n",
        "\n",
        "# Make a directory in your google drive namde \"MICCAI_2020_reg_tutorial\"\n",
        "if not os.path.exists(\"./MICCAI_2020_reg_tutorial\"):\n",
        "  os.makedirs(\"./MICCAI_2020_reg_tutorial\")\n",
        "# Move into the dir\n",
        "%cd ./MICCAI_2020_reg_tutorial\n",
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MICCAI_2020_reg_tutorial\n",
            "/content/MICCAI_2020_reg_tutorial\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCMVBQEKnrNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Clone the DeepReg repository which contains the code\n",
        "! git clone https://github.com/DeepRegNet/DeepReg.git\n",
        "%cd ./DeepReg/\n",
        "# pip install into the notebook env\n",
        "! pip install -e .\n",
        "# ! pip install wget\n",
        "# ! pip install pyyaml==5.1\n",
        "! pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW7CCpamNZwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec73ecd1-6705-4ee8-cc63-b6bd383e93da"
      },
      "source": [
        "import yaml\n",
        "yaml.__version__\n",
        "! pip install -U PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: PyYAML in /usr/local/lib/python3.6/dist-packages (5.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn1IiKXOrpmJ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Registration <a name=\"IntroReg\"></a>\n",
        "\n",
        "Image registration is an essential process in many clinical applications and computer assisted interventions [1, 11]. \n",
        "\n",
        "Applications of medical image registration include - but are not limited to:\n",
        "* Multi-modal registration for image guided surgery: for example, aligning real-time ultrasound scans to pre-operative CT or MRI scans to real-time achieve guidance in neurosurgical or abdominal applications [2-3].\n",
        "* Atlas-based image segmentation: aligning new images to those carefully segmented, such that the reference segmentations can be propagated to those new images [4].\n",
        "* Longitudinal comparison of images for a given patient with the same imaging modality: for example, comparing the outcome of given cancer treatment in a patients' CT scans over time [5].\n",
        "* Inter-subject comparison: for example, a population study of organ shapes [6].\n",
        "\n",
        "\n",
        "Image registration is the mapping of one image coordinate system to another, and can be sub-divided into rigid registrations and non-rigid registrations, depending on whether or not higher-dimesional tissue deformations is modelled as oppose to, for exmaple, a 6 degree-of-freedom (3 translational axes + 3 rotational axes) rigid transformation. Data may be aligned in many ways - spatially or temporally being two key ones.\n",
        "\n",
        "Typically, we refer to one of the images in the pair as the *moving* image and the other as the *fixed* image. The goal is to find the *correspondence* that aligns the moving image to the fixed image - the transform will project the *moving* coordinates into the *fixed* coordinate space. The correspondence specifies the mapping between all voxels from one image to those from another image. The correspondence can be represented by a dense displacment field (DDF) [9], defined as a set of displacement vectors for all pixels or voxels from one image to another. By using these displacement vectors, one image can be \"warped\" to become more \"similar\" to another.\n",
        "\n",
        "\n",
        "## Classical Registration Methods\n",
        "Image registration has been an active area of research for decades. Historically, the so-called classical algorithms pose registration as an optimisation problem between a given pair of moving and fixed images. In these methods, a pre-defined transformation model, rigid or nonrigid, is iteratively optimised to minimise a similarty measure - a metric that quantifies how \"similar\" the warped moving image and the fixed image are. \n",
        "\n",
        "The similarity measure can be a function sampling only important image features (extracted from a pre-processing step) or directly sampling all intensity values from both images:\n",
        "\n",
        "* **Feature-based registration**: for example, a transformation between point clouds - a type of features widely used in many applications - can be estimated using Iterative Closest Point (ICP) [7] or coherent point drift [8] (CPD), for rigid or nonrigid transformation, respectively. The basis of ICP is to iteratively minimise the distance between the two point clouds by matching the points from one set to the closest point in the other set. This is done by searching for the minimum Euclidean distance for each point to the other cloud(otherwise known as the L2 distance), simply, $$L_2 = \\sqrt(\\vec{x_1} - \\vec{x_2})^2$$ The transformation can be estimated from the found set of pairs, or correspondence, applying the transformation, and repeating the process many times.\n",
        "\n",
        "  Another type of features that may be used are additional labels to highlight specific regions of interest (ROIs) to the clinician in the scan(s) - *segmentations*. These are typically provided as binary masks of organs that match the image dimensions, and refer to specific images or sets of images. While these data can be used to compute other types of similarity measures (like overlap measures, such as a Dice score) to drive a registration algorithm, they may be considered as weak labels to drive a learning-based registration algorithm - therefore we refer to these segmentations as labels in this tutorial.\n",
        "\n",
        "* **Intensity-based registration**: Typically, medical imaging data does not come in point cloud format, but rather, 2D, 3D and 4D matrices with a range of intensity values at each pixel or voxel. As such, different measures can be used directly on the intensity distributions of the data to measure similarity between the data. Examples of measures are cross correlation, mutual information and simple sum-square-difference - these intensity-based algorithms can optimise a transformation model directly using images without the feature extraction step.\n",
        "\n",
        "Many of today's deep-learning-based methods have been heavily influenced and derived from these prior areas of research.\n",
        "\n",
        "\n",
        "## Why use Deep Learning for Medical Image Registration?\n",
        "Usually, classical methods are unable to handle real-time registration of large point sets or feature intensity distributions owing to their computationally intense nature, especially in the case of 3D nonrigid registration. State-of-the-art classical methods are implemented on GPU still struggle for real-time performance for many time-critical clinical applications.\n",
        "\n",
        "Secondly, classical algorithms are inherently pairwise approaches that can not take into account population data statistics directly and relying on well-designed transformation models and valid similarity being avaialable and robust, challenging for many real-world tasks.\n",
        "\n",
        "In contrast, the computationally efficient inference and the ability to model complex, non-linear transformations of learning-based methods has motivated the development of neural networks which infer the optimal transformation from unseen data [1]. \n",
        "\n",
        "However, it is important to point out that \n",
        "* Many deep-learning-based methods are still subject to these limitations, especially those that borrow transformation models and similarity measures directly from the classical methods\n",
        "* Deep learning models are limited at inference time by how the model was trained - it is well known that deep learning models can overfit to the training data.\n",
        "* Classical algorithms have been refined for many clinical applications and still work really well.\n",
        "\n",
        "# Image Registration with Deep Learning <a name=\"DeepRegistrationIntro\"></a>\n",
        "\n",
        "In recent years, image registration has been re-formulated as a learning problem, in which, pairs of moving and fixed images are passed to a machine learning model (usually a neural network nowadays) to predict a transformation between the images.\n",
        "\n",
        "In this tutorial, we investigate three factors that determine a deep learning approach for image registration:\n",
        "\n",
        "1. What type of transformation is one trying to predict?\n",
        "2. What type image data are being registered? Are there any other data, such as segmentations, to support the registration?\n",
        "3. Are the data paired? Are they labeled?\n",
        "\n",
        "\n",
        "## Types of transformations\n",
        "\n",
        "We need to choose what type of transformation we want to predict. \n",
        "\n",
        "- **Predicting a dense displacement field**\n",
        "\n",
        "  Given a pair of moving and fixed images, a registration network\n",
        "  can be trained to output dense displacement field (DDF)  [9] of the same shape as the moving image. Each value in the DDF\n",
        "  can be considered as the placement of the corresponding pixel / voxel of the moving\n",
        "  image. Therefore, the DDF defines a mapping from the moving image's coordinates to the\n",
        "  fixed image.\n",
        "\n",
        "  In this tutorial, we mainly focus on DDF-based methods.\n",
        "\n",
        "- **Predict a dense velocity field**\n",
        "\n",
        "  Another option is to predict a dense velocity field (DVF) between a pair of images, such that a diffeomorphic\n",
        "  DDF can be numerically integrated. We refer you to [9]\n",
        "  for more details.\n",
        "\n",
        "- **Predict an affine transformation**\n",
        "\n",
        "  A more constrained option is to predict an affine transformation, parameterised the\n",
        "  affine transformation matrix of 12 degrees of freedom. The DDF can then be computed to\n",
        "  resample the moving images in fixed image space.\n",
        "\n",
        "- **Predict a region of interest**\n",
        "\n",
        "  Instead of outputting the transformation between coordinates, given moving image,\n",
        "  fixed image, and a region of interest (ROI) in the moving image, the network can\n",
        "  predict the ROI in fixed image directly. Interested readers are referred to the MICCAI\n",
        "  2019 paper [10].\n",
        "\n",
        "\n",
        "## Data availability, level of supervision and network training strategies\n",
        "\n",
        "Depending on the availability of the data labels, registration networks can be trained\n",
        "with different approaches. These will influence our loss choice.\n",
        "\n",
        "### Unsupervised\n",
        "\n",
        "When the data label is unavailable, training can be achieved via an unsupervised loss.\n",
        "The\n",
        "following is an illustration of an unsupervised DDF-based registration network.\n",
        "\n",
        "![Unsupervised DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-unsupervised.svg)\n",
        "\n",
        "The loss function often consists of the intensity based loss and deformation loss.\n",
        "\n",
        "### Weakly-supervised\n",
        "\n",
        "When there is no intensity based loss that is appropriate for the image pair one would\n",
        "like to register, the training can take a pair of corresponding moving and fixed labels\n",
        "(in addition to the image pair), represented by binary masks, to compute a label\n",
        "dissimilarity (feature based loss) to drive the registration.\n",
        "\n",
        "Combined with the regularisation on the predicted displacement field, this forms a\n",
        "weakly-supervised training. An illustration of an weakly-supervised DDF-based\n",
        "registration network is provided below.\n",
        "\n",
        "When multiple labels are available for each image, the labels can be sampled during\n",
        "training, such that only one label per image is used in each iteration of the\n",
        "data set (epoch). We expand on this in the [DeepReg data sampling API](tutorial_sampling.md) but do not need this for the tutorials in this notebook.\n",
        "\n",
        "![Weakly-supervised DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-weakly-supervised.svg)\n",
        "\n",
        "### Combined\n",
        "\n",
        "When the data label is available, combining intensity based, feature based, and\n",
        "deformation based losses together has shown superior registration accuracy, compared to\n",
        "unsupervised and weakly supervised methods. Following is an illustration of a combined\n",
        "DDF-based registration network.\n",
        "\n",
        "![Combined DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-combined.svg)\n",
        "\n",
        "## Loss functions\n",
        "\n",
        "We aim to train a network to predict some transformation between a pair of images that is likely. To do this, we need to define what is a \"likely\" transformation. This is done via a *loss function*.\n",
        "\n",
        "The loss function defined to train a registration network will depend on the type of data we have access to.\n",
        "\n",
        "- **Intensity based (image based) loss**\n",
        "\n",
        "  This type of loss measures the dissimilarity of the fixed image and warped moving\n",
        "  image, which is adapted from the classical image registration methods. Intensity based\n",
        "  loss is modality-independent and similar to many other well-studied computer vision\n",
        "  and medical imaging tasks, such as image segmentation.\n",
        "\n",
        "  The common loss functions are normalized cross correlation (NCC), sum of squared\n",
        "  distance (SSD), and normalized mutual information (MI).\n",
        "\n",
        "- **Feature based (label based) loss**\n",
        "\n",
        "  Provided labels for the input images, a feature based loss may be used to measure the (dis)similarity of warped regions of interest. Having computed a transformation between images using the net, one of the labels is warped and compared to the ground truth image label.\n",
        " Labels are typically manually contoured organs.\n",
        "\n",
        "  The common loss function is Dice loss, Jacard and average cross-entropy over all\n",
        "  voxels, which are measures of the overlap of the ROIs. The loss will minimise the negative overlap measure (eg. l = 1 - dice_score) to maximise overlap of the regions during training.\n",
        "\n",
        "- **Deformation loss**\n",
        "\n",
        "  Additionally, training may be regularised by computing the \"likelihood\" of a given displacement field. High deformation losses point to very unlikely displacement due to high gradients of the field - typically, deformation losses ensure smoothness in the displacement field. For DDFs, typical regularisation losses are bending energy losses, L1 or L2 norms of the displacement gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRbUdZjq0_A",
        "colab_type": "text"
      },
      "source": [
        "## Image Registration with Deep Learning: Summary <a name=\"DeepRegistrationIntro\"></a>\n",
        "\n",
        "For deep learning methods, pairs of images, denoted as moving\n",
        "and fixed images, are passed to the network to predict a transformation between the images.\n",
        "\n",
        "The deep learning approach for medical image registration will depend on mainly three factors:\n",
        "\n",
        "1. What type of transformation is one trying to predict?\n",
        "2. What type image data are being registered? Are there any other data, such as segmentations, to support the registration?\n",
        "3. Are the data paired? Are they labeled?\n",
        "\n",
        "From this, we can design an appropriate architecture and choose an adequate loss function to motivate training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC-lMrB4Ct_",
        "colab_type": "text"
      },
      "source": [
        "# Classical Registration Examples <a name=\"classical-examples\"></a>\n",
        "\n",
        "We will define \"single-layer\" examples using functions from DeepReg, and train it to register two images.\n",
        "\n",
        "First, we will illustrate the possibility of \"self-registering\" an image to it's affine-transformed counterpart, using head and neck CT scans data [12]. The data is open source, the original source can be found [here.](https://wiki.cancerimagingarchive.net/display/Public/Head-Neck-PET-CT)\n",
        "\n",
        "\n",
        "Then, we will nonrigid-register inter-subject scans, using MR images from two prostate cancer patients [13]. The data is from the [PROMISE12 Grand Challenge](https://promise12.grand-challenge.org/).\n",
        "\n",
        "## Learning an affine transformation: a \"self-registration\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNdefjurAmQ",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# We import some utility modules.\n",
        "import nibabel\n",
        "import tensorflow as tf \n",
        "import deepreg.model.layer as layer\n",
        "import deepreg.model.loss.image as image_loss\n",
        "import deepreg.model.loss.deform as deform_loss\n",
        "import deepreg.model.layer_util as layer_util\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# We set the plot size to some parameters.\n",
        "plt.rcParams[\"figure.figsize\"] = (100,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAh-31ZOO5Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define some utility functions first\n",
        "## optimisation\n",
        "@tf.function\n",
        "def train_step_CT(grid, weights, optimizer, mov, fix):\n",
        "    \"\"\"\n",
        "    Train step function for backprop using gradient tape\n",
        "    :param grid: reference grid return from util.get_reference_grid\n",
        "    :param weights: trainable affine parameters [1, 4, 3]\n",
        "    :param optimizer: tf.optimizers\n",
        "    :param mov: moving image [1, m_dim1, m_dim2, m_dim3]\n",
        "    :param fix: fixed image [1, f_dim1, f_dim2, f_dim3]\n",
        "    :return loss: image dissimilarity to minimise\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = layer_util.resample(vol=mov, loc=layer_util.warp_grid(grid, weights))\n",
        "        loss = image_loss.dissimilarity_fn(\n",
        "            y_true=fix, y_pred=pred, name=image_loss_name\n",
        "        )\n",
        "    gradients = tape.gradient(loss, [weights])\n",
        "    optimizer.apply_gradients(zip(gradients, [weights]))\n",
        "    return loss\n",
        "\n",
        "def plot_results(moving_image, fixed_image, warped_moving_image, nIdx):\n",
        "  \"\"\"\n",
        "  Plotting the results from training\n",
        "  :param moving_image: [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param fixed_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param warped_moving_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param nIdx: number of indices to display\n",
        "  \"\"\"\n",
        "  # Display\n",
        "  plt.figure()\n",
        "  for idx in range(nIdx):\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 1)\n",
        "      axs.imshow(moving_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 2)\n",
        "      axs.imshow(fixed_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 3)\n",
        "      axs.imshow(warped_moving_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "  plt.ion()\n",
        "  plt.suptitle('Moving Image - Fixed Image - Warped Moving Image', fontsize=200)\n",
        "  plt.show()\n",
        "\n",
        "def display(moving_image, fixed_image):\n",
        "  \"\"\"\n",
        "  Displaying our two image tensors to register\n",
        "  :param moving_image: [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param fixed_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  \"\"\"\n",
        "  # Display\n",
        "  idx_slices = [int(5+x*5) for x in range(int(fixed_image_size[3]/5)-1)]\n",
        "  nIdx = len(idx_slices)\n",
        "  plt.figure()\n",
        "  for idx in range(len(idx_slices)):\n",
        "      axs = plt.subplot(nIdx, 2, 2*idx+1)\n",
        "      axs.imshow(moving_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "      axs.axis('off')\n",
        "      axs = plt.subplot(nIdx, 2, 2*idx+2)\n",
        "      axs.imshow(fixed_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "      axs.axis('off')\n",
        "  plt.suptitle('Moving Image - Fixed Image', fontsize=200)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN-Lal7TMzZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9a5f1ae4-4e49-4ce1-90a1-4143913966c3"
      },
      "source": [
        "# We download the data first\n",
        "MAIN_PATH = os.getcwd()\n",
        "PROJECT_DIR = os.path.join(MAIN_PATH, \"demos/classical_ct_headandneck_affine\")\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "FILE_PATH = os.path.abspath(os.path.join(PROJECT_DIR, DATA_PATH, \"demo.h5\"))\n",
        "ORIGIN = \"https://github.com/YipengHu/example-data/raw/master/hnct/demo.h5\"\n",
        "\n",
        "if os.path.exists(FILE_PATH):\n",
        "    os.remove(FILE_PATH)\n",
        "else:\n",
        "  os.makedirs(os.path.join(PROJECT_DIR, DATA_PATH))\n",
        "get_file(FILE_PATH, ORIGIN)\n",
        "print(\"CT head-and-neck data downloaded: %s.\" % FILE_PATH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/YipengHu/example-data/raw/master/hnct/demo.h5\n",
            "2433024/2426880 [==============================] - 0s 0us/step\n",
            "CT head-and-neck data downloaded: /content/MICCAI_2020_reg_tutorial/demos/classical_ct_headandneck_affine/dataset/demo.h5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlvkyqKjNrRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "b9961b10-e592-4d7c-f25b-5720f1af5041"
      },
      "source": [
        "## registration parameters\n",
        "image_loss_name = \"ssd\"\n",
        "learning_rate = 0.01\n",
        "total_iter = int(1000)\n",
        "\n",
        "\n",
        "# Opening the file\n",
        "fid = h5py.File(FILE_PATH, \"r\")\n",
        "fixed_image = tf.cast(tf.expand_dims(fid[\"image\"], axis=0), dtype=tf.float32)\n",
        "\n",
        "# normalisation to [0,1]\n",
        "fixed_image = (fixed_image - tf.reduce_min(fixed_image)) / (\n",
        "    tf.reduce_max(fixed_image) - tf.reduce_min(fixed_image)\n",
        ")  \n",
        "\n",
        "# generate a radomly-affine-transformed moving image using DeepReg utils\n",
        "fixed_image_size = fixed_image.shape\n",
        "# The following function generates a random transform.\n",
        "transform_random = layer_util.random_transform_generator(batch_size=1, scale=0.2)\n",
        "\n",
        "# We create a reference grid of image size\n",
        "grid_ref = layer_util.get_reference_grid(grid_size=fixed_image_size[1:4])\n",
        "\n",
        "# We warp our reference grid by our random transform\n",
        "grid_random = layer_util.warp_grid(grid_ref, transform_random)\n",
        "# We resample the fixed image with the random transform to create a distorted\n",
        "# image, which we will use as our moving image.\n",
        "moving_image = layer_util.resample(vol=fixed_image, loc=grid_random)\n",
        "\n",
        "# warp the labels to get ground-truth using the same random affine transform\n",
        "# for validation\n",
        "fixed_labels = tf.cast(tf.expand_dims(fid[\"label\"], axis=0), dtype=tf.float32)\n",
        "# We have multiple labels, so we apply the transform to all the labels by\n",
        "# stacking them\n",
        "moving_labels = tf.stack(\n",
        "    [\n",
        "        layer_util.resample(vol=fixed_labels[..., idx], loc=grid_random)\n",
        "        for idx in range(fixed_labels.shape[4])\n",
        "    ],\n",
        "    axis=4,\n",
        ")\n",
        "\n",
        "\n",
        "# We create an affine transformation as a trainable weight layer\n",
        "var_affine = tf.Variable(\n",
        "    initial_value=[\n",
        "        [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]\n",
        "    ],\n",
        "    trainable=True,\n",
        ")\n",
        "\n",
        "# We perform an optimisation by backpropagating the loss through to our \n",
        "# trainable weight layer.\n",
        "optimiser = tf.optimizers.Adam(learning_rate)\n",
        "for step in range(total_iter):\n",
        "    loss_opt = train_step_CT(grid_ref, var_affine, optimiser, moving_image, fixed_image)\n",
        "    if (step % 50) == 0:  # print info\n",
        "        tf.print(\"Step\", step, image_loss_name, loss_opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 ssd [0.0154854683]\n",
            "Step 50 ssd [0.00342396554]\n",
            "Step 100 ssd [0.00320516527]\n",
            "Step 150 ssd [0.00319911726]\n",
            "Step 200 ssd [0.00319555029]\n",
            "Step 250 ssd [0.00319277751]\n",
            "Step 300 ssd [0.00319075]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-18b04174a8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mloss_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_CT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_affine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# print info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_loss_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzp0FEz1Qg9Q",
        "colab_type": "text"
      },
      "source": [
        "Once the optimisation converges, we can use the optimised affine transformation to warp the moving images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc8vc_rCPdSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## warp the moving image using the optimised affine transformation\n",
        "grid_opt = layer_util.warp_grid(grid_ref, var_affine)\n",
        "warped_moving_image = layer_util.resample(vol=moving_image, loc=grid_opt)\n",
        "\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "# display to check the results.\n",
        "plot_results(moving_image, fixed_image, warped_moving_image, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_1SRBlSW5PT",
        "colab_type": "text"
      },
      "source": [
        "We can see especially from the first two slices how the data has registered to the fixed image. Let's see how the transformation appears on the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17lokrA4QV-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check how the labels have been registered\n",
        "warped_moving_labels  = layer_util.resample(vol=moving_labels, loc=grid_opt)\n",
        "\n",
        "# display\n",
        "for idx_label in range(fixed_labels.shape[4]):\n",
        "    plt.figure()\n",
        "    for idx in range(len(idx_slices)):\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 1)\n",
        "        axs.imshow(moving_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\")\n",
        "        axs.axis(\"off\")\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 2)\n",
        "        axs.imshow(fixed_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\")\n",
        "        axs.axis(\"off\")\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 3)\n",
        "        axs.imshow(\n",
        "            warped_moving_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\"\n",
        "        )\n",
        "        axs.axis(\"off\")\n",
        "    plt.ion()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9098lsvHXJ_E",
        "colab_type": "text"
      },
      "source": [
        "Here we can see how in some instances there is a label in the warped moving image versus the original moving image. This is an instance of conditional segmentation.\n",
        "\n",
        "## Learning a nonrigid transformation: an inter-subject registration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TBdx9dDt41n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some utility functions\n",
        "@tf.function\n",
        "def train_step(warper, weights, optimizer, mov, fix):\n",
        "    \"\"\"\n",
        "    Train step function for backpropagation using gradient tape\n",
        "\n",
        "    :param warper: warping function returned from layer.Warping\n",
        "    :param weights: trainable ddf [1, f_dim1, f_dim2, f_dim3, 3]\n",
        "    :param optimizer: tf.optimizers\n",
        "    :param mov: moving image [1, m_dim1, m_dim2, m_dim3]\n",
        "    :param fix: fixed image [1, f_dim1, f_dim2, f_dim3]\n",
        "    :return:\n",
        "        loss: overall loss to optimise\n",
        "        loss_image: image dissimilarity\n",
        "        loss_deform: deformation regularisation\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = warper(inputs=[weights, mov])\n",
        "        # Calculating the image loss between the ground truth and prediction\n",
        "        loss_image = image_loss.dissimilarity_fn(\n",
        "            y_true=fix, y_pred=pred, name=image_loss_name\n",
        "        )\n",
        "        # We calculate the deformation loss\n",
        "        loss_deform = deform_loss.local_displacement_energy(weights, deform_loss_name)\n",
        "        # Total loss is weighted\n",
        "        loss = loss_image + weight_deform_loss * loss_deform\n",
        "    # We calculate the gradients by backpropagating the loss to the trainable layer,\n",
        "    # which for registration is our ddf\n",
        "    gradients = tape.gradient(loss, [weights])\n",
        "    # Using our tf optimizer, we apply the gradients\n",
        "    optimizer.apply_gradients(zip(gradients, [weights]))\n",
        "    return loss, loss_image, loss_deform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXoa6Ac_SLXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We download the data for this example.\n",
        "MAIN_PATH = os.getcwd()\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "if not os.path.exists(os.path.join(MAIN_PATH, DATA_PATH)):\n",
        "  os.makedirs(os.path.join(MAIN_PATH, DATA_PATH))\n",
        "\n",
        "FILE_PATH = os.path.abspath(os.path.join(MAIN_PATH, DATA_PATH, \"demo2.h5\"))\n",
        "ORIGIN = \"https://github.com/YipengHu/example-data/raw/master/promise12/demo2.h5\"\n",
        "\n",
        "get_file(FILE_PATH, ORIGIN)\n",
        "print(\"Prostate MR data downloaded: %s.\" % FILE_PATH)\n",
        "\n",
        "os.chdir(MAIN_PATH)\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "FILE_PATH = os.path.join(MAIN_PATH, DATA_PATH, \"demo2.h5\")\n",
        "\n",
        "fid = h5py.File(FILE_PATH, \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "OPKCDDULPT3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We define some registration parameters - play around with these!\n",
        "image_loss_name = \"lncc\" # local normalised cross correlation loss between images\n",
        "deform_loss_name = \"bending\" # Loss to measure the bending energy of the ddf\n",
        "weight_deform_loss = 10 # we weight the deformation loss\n",
        "learning_rate = 0.1\n",
        "total_iter = int(3000) # This will train for longer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNk5DbimPT3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We get our two subject images from our datasets\n",
        "moving_image = tf.cast(tf.expand_dims(fid[\"image0\"], axis=0), dtype=tf.float32)\n",
        "fixed_image = tf.cast(tf.expand_dims(fid[\"image1\"], axis=0), dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYo61smNPT30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We initialise our layers\n",
        "fixed_image_size = fixed_image.shape\n",
        "\n",
        "# Creating our DDF tensor that can be trained\n",
        "# The DDF will be of shape [IM_SIZE_1, IM_SIZE_2, 3],\n",
        "# representing the displacement field at each dimension.\n",
        "var_ddf = tf.Variable(initialiser(fixed_image_size + [3]), name=\"ddf\", trainable=True)\n",
        "\n",
        "# We create a warping layer and initialise an optimizer\n",
        "warping = layer.Warping(fixed_image_size=fixed_image_size[1:4])\n",
        "optimiser = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "## Training the layer\n",
        "## With GPU this takes about 5 minutes.\n",
        "for step in range(total_iter):\n",
        "    # Call the gradient tape function\n",
        "    loss_opt, loss_image_opt, loss_deform_opt = train_step(\n",
        "        warping, var_ddf, optimiser, moving_image, fixed_image\n",
        "    )\n",
        "    if (step % 50) == 0:  # print info at every 50th step\n",
        "        tf.print(\n",
        "            \"Step\",\n",
        "            step,\n",
        "            \"loss\",\n",
        "            loss_opt,\n",
        "            image_loss_name,\n",
        "            loss_image_opt,\n",
        "            deform_loss_name,\n",
        "            loss_deform_opt,\n",
        "        )\n",
        "        # Visualising loss during training\n",
        "        # plt.figure()\n",
        "        # fig, axs = plt.subplots(1, 3)\n",
        "        # warped_moving_image = warping(inputs=[var_ddf, moving_image])\n",
        "        # axs[0].imshow(moving_image[0, ..., 12])\n",
        "        # axs[1].imshow(fixed_image[0, ..., 12])\n",
        "        # axs[2].imshow(warped_moving_image[0, ..., 12])\n",
        "        # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCBZ1DXHPT32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Warp the moving image using the optimised ddf and the warping layer.\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "warped_moving_image = warping(inputs=[var_ddf, moving_image])\n",
        "plot_results(moving_image, fixed_image, warped_moving_image, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dArjLUqbY_JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We can observe the effects of the warping on the moving label using\n",
        "# the optimised affine transformation\n",
        "moving_label = tf.cast(tf.expand_dims(fid[\"label0\"], axis=0), dtype=tf.float32)\n",
        "fixed_label = tf.cast(tf.expand_dims(fid[\"label1\"], axis=0), dtype=tf.float32)\n",
        "\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "warped_moving_labels = warping(inputs=[var_ddf, moving_label])\n",
        "plot_results(moving_label, fixed_label, warped_moving_labels, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPAH2zDq4BjB",
        "colab_type": "text"
      },
      "source": [
        "# Medical Image registration using an adapted DeepReg Demo <a name=\"deep-example\"></a>\n",
        "\n",
        "Now, we will build a more complex demo to investigate a clinical case, using deep-learning.\n",
        "\n",
        "This is a registration between ct images acquired at different time points for a single patient. The images being registered are taken at inspiration and expiration for each subject. This is an intra subject registration. This type of intra subject registration is useful when there is a need to track certain features on a medical image such as tumor location when conducting invasive procedures [14]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuxx6u164ABI",
        "colab_type": "text"
      },
      "source": [
        "The data files used in this tutorial have been pre-arranged in a folder, required by the DeepReg paired dataset loader, and can be downloaded as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6kkjw4-JdPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "20e3fefe-bab6-4de9-f434-a6ef2b004dfa"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "! pip install wget\n",
        "import wget\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "! pwd\n",
        "\n",
        "main_path = os.getcwd()\n",
        "\n",
        "if main_path is not \"/content/MICCAI_2020_reg_tutorial/DeepReg/\":\n",
        "  main_path = \"/content/MICCAI_2020_reg_tutorial/DeepReg/\"\n",
        "######## DOWNLOADING AND UNZIPPING ALL FILES INTO CORRECT PATH ########\n",
        "\n",
        "project_dir = \"demos/paired_ct_lung\"\n",
        "os.chdir(os.path.join(main_path, project_dir))\n",
        "\n",
        "url = \"https://zenodo.org/record/3835682/files/training.zip\"\n",
        "\n",
        "# if wget is installed remove following line from comments and comment\n",
        "# out the fname = 'training.zip' line\n",
        "fname = wget.download(url)\n",
        "\n",
        "print(\"The file \", fname, \" has successfully been downloaded!\")\n",
        "\n",
        "data_folder_name = r\"data\"\n",
        "\n",
        "if os.path.exists(os.path.join(main_path, project_dir, data_folder_name)) is not True:\n",
        "    os.mkdir(os.path.join(main_path, project_dir, data_folder_name))\n",
        "\n",
        "with zipfile.ZipFile(fname, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(os.path.join(main_path, project_dir, data_folder_name))\n",
        "\n",
        "print(\"Files unzipped!\")\n",
        "\n",
        "os.remove(fname)\n",
        "os.chdir(main_path)\n",
        "\n",
        "######## MOVING FILES INTO TRAIN DIRECTORY ########\n",
        "\n",
        "path_to_data_folder = os.path.join(main_path, project_dir, data_folder_name)\n",
        "path_to_train = os.path.join(main_path, project_dir, data_folder_name, \"train\")\n",
        "path_to_test = os.path.join(main_path, project_dir, data_folder_name, \"test\")\n",
        "path_to_images_and_labels = os.path.join(\n",
        "    main_path, project_dir, data_folder_name, \"training\"\n",
        ")\n",
        "\n",
        "labels_fnames = os.listdir(os.path.join(path_to_images_and_labels, \"lungMasks\"))\n",
        "images_fnames = os.listdir(os.path.join(path_to_images_and_labels, \"scans\"))\n",
        "\n",
        "if os.path.exists(path_to_train) is not True:\n",
        "    os.mkdir(path_to_train)\n",
        "    os.mkdir(os.path.join(path_to_train, \"fixed_images\"))\n",
        "    os.mkdir(os.path.join(path_to_train, \"fixed_labels\"))\n",
        "    os.mkdir(os.path.join(path_to_train, \"moving_images\"))\n",
        "    os.mkdir(os.path.join(path_to_train, \"moving_labels\"))\n",
        "\n",
        "\n",
        "def moveFilesIntoCorrectPath(\n",
        "    fnames, path_to_images_and_labels, new_path, suffix, sub_folder_name\n",
        "):\n",
        "    os.chdir(os.path.join(path_to_images_and_labels, sub_folder_name))\n",
        "    for file in fnames:\n",
        "        if \"insp\" in file:\n",
        "            source = file\n",
        "            destination = os.path.join(path_to_train, \"fixed_\" + suffix)\n",
        "            shutil.move(source, destination)\n",
        "        if \"exp\" in file:\n",
        "            source = file\n",
        "            destination = os.path.join(path_to_train, \"moving_\" + suffix)\n",
        "            shutil.move(source, destination)\n",
        "\n",
        "\n",
        "if os.path.exists(path_to_images_and_labels):\n",
        "\n",
        "    moveFilesIntoCorrectPath(\n",
        "        images_fnames, path_to_images_and_labels, path_to_train, \"images\", \"scans\"\n",
        "    )\n",
        "    moveFilesIntoCorrectPath(\n",
        "        labels_fnames, path_to_images_and_labels, path_to_train, \"labels\", \"lungMasks\"\n",
        "    )\n",
        "\n",
        "os.chdir(main_path)\n",
        "\n",
        "######## MOVING FILES INTO TEST AND VALID DIRECTORY ########\n",
        "\n",
        "path_to_test = os.path.join(path_to_data_folder, \"test\")\n",
        "path_to_valid = os.path.join(path_to_data_folder, \"valid\")\n",
        "\n",
        "if os.path.exists(path_to_test) is not True:\n",
        "\n",
        "    os.mkdir(path_to_test)\n",
        "    os.mkdir(os.path.join(path_to_test, \"fixed_images\"))\n",
        "    os.mkdir(os.path.join(path_to_test, \"fixed_labels\"))\n",
        "    os.mkdir(os.path.join(path_to_test, \"moving_images\"))\n",
        "    os.mkdir(os.path.join(path_to_test, \"moving_labels\"))\n",
        "\n",
        "    ratio_of_test_and_valid_samples = 0.2\n",
        "\n",
        "    unique_case_names = []\n",
        "    for file in images_fnames:\n",
        "        case_name_as_list = file.split(\"_\")[0:2]\n",
        "        case_name = case_name_as_list[0] + \"_\" + case_name_as_list[1]\n",
        "        unique_case_names.append(case_name)\n",
        "    unique_case_names = np.unique(unique_case_names)\n",
        "\n",
        "    test_and_valid_cases = random.sample(\n",
        "        list(unique_case_names),\n",
        "        int(ratio_of_test_and_valid_samples * len(unique_case_names)),\n",
        "    )\n",
        "    test_cases = test_and_valid_cases[\n",
        "        0 : int(int(ratio_of_test_and_valid_samples * len(unique_case_names) / 2))\n",
        "    ]\n",
        "    valid_cases = test_and_valid_cases[\n",
        "        int(int(ratio_of_test_and_valid_samples * len(unique_case_names) / 2)) + 1 :\n",
        "    ]\n",
        "\n",
        "    def moveTestCasesIntoCorrectPath(test_cases, path_to_train, path_to_test):\n",
        "        folder_names = os.listdir(path_to_train)\n",
        "        os.chdir(path_to_train)\n",
        "        for case in test_cases:\n",
        "            for folder in folder_names:\n",
        "                file_names = os.listdir(os.path.join(path_to_train, folder))\n",
        "                for file in file_names:\n",
        "                    if case in file:\n",
        "                        os.chdir(os.path.join(path_to_train, folder))\n",
        "                        source = file\n",
        "                        destination = os.path.join(path_to_test, folder)\n",
        "                        shutil.move(source, destination)\n",
        "\n",
        "    moveTestCasesIntoCorrectPath(test_cases, path_to_train, path_to_test)\n",
        "\n",
        "    os.mkdir(path_to_valid)\n",
        "    os.mkdir(os.path.join(path_to_valid, \"fixed_images\"))\n",
        "    os.mkdir(os.path.join(path_to_valid, \"fixed_labels\"))\n",
        "    os.mkdir(os.path.join(path_to_valid, \"moving_images\"))\n",
        "    os.mkdir(os.path.join(path_to_valid, \"moving_labels\"))\n",
        "\n",
        "    moveTestCasesIntoCorrectPath(valid_cases, path_to_train, path_to_valid)\n",
        "\n",
        "######## NAMING FILES SUCH THAT THEIR NAMES MATCH FOR PAIRING ########\n",
        "\n",
        "# name all files such that names match exactly for training\n",
        "\n",
        "for folder in os.listdir(path_to_train):\n",
        "    path_to_folder = os.path.join(path_to_train, folder)\n",
        "    os.chdir(path_to_folder)\n",
        "    for file in os.listdir(path_to_folder):\n",
        "        if \"_insp\" in file:\n",
        "            new_name = file.replace(\"_insp\", \"\")\n",
        "        elif \"_exp\" in file:\n",
        "            new_name = file.replace(\"_exp\", \"\")\n",
        "        source = file\n",
        "        destination = new_name\n",
        "        os.rename(source, destination)\n",
        "\n",
        "# name all files such that names match exactly for testing\n",
        "\n",
        "for folder in os.listdir(path_to_test):\n",
        "    path_to_folder = os.path.join(path_to_test, folder)\n",
        "    os.chdir(path_to_folder)\n",
        "    for file in os.listdir(path_to_folder):\n",
        "        if \"_insp\" in file:\n",
        "            new_name = file.replace(\"_insp\", \"\")\n",
        "        elif \"_exp\" in file:\n",
        "            new_name = file.replace(\"_exp\", \"\")\n",
        "        source = file\n",
        "        destination = new_name\n",
        "        os.rename(source, destination)\n",
        "\n",
        "# name all files such that names match exactly for validation\n",
        "\n",
        "for folder in os.listdir(path_to_valid):\n",
        "    path_to_folder = os.path.join(path_to_valid, folder)\n",
        "    os.chdir(path_to_folder)\n",
        "    for file in os.listdir(path_to_folder):\n",
        "        if \"_insp\" in file:\n",
        "            new_name = file.replace(\"_insp\", \"\")\n",
        "        elif \"_exp\" in file:\n",
        "            new_name = file.replace(\"_exp\", \"\")\n",
        "        source = file\n",
        "        destination = new_name\n",
        "        os.rename(source, destination)\n",
        "\n",
        "print(\"All files moved and restructured\")\n",
        "\n",
        "shutil.rmtree(os.path.join(path_to_images_and_labels))\n",
        "os.chdir(main_path)\n",
        "\n",
        "######## NOW WE NEED TO RESCALE EACH IMAGE ########\n",
        "\n",
        "data_dir = r\"demos/paired_ct_lung/data\"\n",
        "folders = os.listdir(data_dir)\n",
        "\n",
        "for folder in folders:\n",
        "    subfolders = os.listdir(os.path.join(data_dir, folder))\n",
        "    print(\"\\n Working on \", folder, \", progress:\")\n",
        "    for subfolder in tqdm(subfolders):\n",
        "        files = os.listdir(os.path.join(data_dir, folder, subfolder))\n",
        "        for file in files:\n",
        "            if file.startswith(\"case_020\"):  # this case did not laod correctly\n",
        "                os.remove(os.path.join(data_dir, folder, subfolder, file))\n",
        "            else:\n",
        "                im_data = np.asarray(\n",
        "                    nib.load(os.path.join(data_dir, folder, subfolder, file)).dataobj,\n",
        "                    dtype=np.float32,\n",
        "                )\n",
        "                if np.max(im_data) > 255.0:\n",
        "                    im_data = ((im_data + 285) / (3770 + 285)) * 255.0  # rescale image\n",
        "                    img = nib.Nifti1Image(im_data, affine=None)\n",
        "                    nib.save(img, os.path.join(data_dir, folder, subfolder, file))\n",
        "                    if np.max(img.dataobj) > 255.0:\n",
        "                        print(\n",
        "                            \"Recheck the following file: \",\n",
        "                            os.path.join(data_dir, folder, subfolder, file),\n",
        "                        )\n",
        "                    nib.save(img, os.path.join(data_dir, folder, subfolder, file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "/content/MICCAI_2020_reg_tutorial/DeepReg\n",
            "The file  training.zip  has successfully been downloaded!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files unzipped!\n",
            "All files moved and restructured\n",
            "\n",
            " Working on  train , progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:32<00:00,  8.01s/it]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Working on  test , progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.58it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Working on  valid , progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  8.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d7UDHo7675w",
        "colab_type": "text"
      },
      "source": [
        "The number of epochs to train for can be changed by changing num_epochs. The default is 2 epochs but training for longer will improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyxpCCVtdDj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f0dff129-333e-4839-fc67-c1630364ea1d"
      },
      "source": [
        "import os\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "path_to_file = r'deepreg/config/test'\n",
        "\n",
        "filename = 'ddf.yaml'\n",
        "file = open(os.path.join(path_to_file, filename)).read().splitlines()\n",
        "file[41] = file[41][:-2] + ' ' + str(num_epochs)\n",
        "        \n",
        "open(os.path.join(path_to_file, filename), 'w').write('\\n'.join(file))\n",
        "\n",
        "new_file = open(os.path.join(path_to_file, filename)).read().splitlines()\n",
        "print('Line changed to: \\n', new_file[41])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line changed to: \n",
            "   epochs: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJmrcRhh5RpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "54abb261-14a1-439c-faae-734843719181"
      },
      "source": [
        "from deepreg.train import train\n",
        "import tensorflow as tf\n",
        "\n",
        "######## NOW WE DO THE TRAINING ########\n",
        "tf.test.gpu_device_name()\n",
        "print(tf.config.experimental.list_physical_devices('GPU'))\n",
        "gpu = \"\"\n",
        "gpu_allow_growth = False\n",
        "ckpt_path = \"\"\n",
        "log_dir = \"learn2reg_t2_paired_train_logs\"\n",
        "config_path = [\n",
        "    r\"deepreg/config/test/ddf.yaml\",\n",
        "    r\"demos/paired_ct_lung/paired_ct_lung.yaml\",\n",
        "]\n",
        "train(\n",
        "    gpu=gpu,\n",
        "    config_path=config_path,\n",
        "    gpu_allow_growth=gpu_allow_growth,\n",
        "    ckpt_path=ckpt_path,\n",
        "    log_dir=log_dir,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Log directory logs/learn2reg_t2_paired_train_logs exists already.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "8/8 [==============================] - 143s 18s/step - metric/foreground_pred: 0.1598 - loss/weighted_regularization: 9.3284e-09 - metric/dice_float: 0.6346 - metric/foreground_label: 0.1970 - loss/label_dissimilarity: 0.4746 - metric/dice_binary: 0.6350 - loss/image_dissimilarity: -0.3270 - loss/weighted_image_dissimilarity: -0.0327 - metric/tre: 20.2357 - loss/weighted_label_dissimilarity: 0.4746 - loss: 0.4419 - loss/regularization: 1.8657e-08 - val_metric/foreground_pred: 0.1277 - val_loss/weighted_regularization: 6.6453e-11 - val_metric/dice_float: 0.9043 - val_metric/foreground_label: 0.1419 - val_loss/label_dissimilarity: 0.3303 - val_metric/dice_binary: 0.9043 - val_loss/image_dissimilarity: -0.4434 - val_loss/weighted_image_dissimilarity: -0.0443 - val_metric/tre: 2.4321 - val_loss/weighted_label_dissimilarity: 0.3303 - val_loss: 0.2860 - val_loss/regularization: 1.3291e-10\n",
            "Epoch 2/2\n",
            "8/8 [==============================] - 147s 18s/step - metric/foreground_pred: 0.1605 - loss/weighted_regularization: 8.2773e-08 - metric/dice_float: 0.6495 - metric/foreground_label: 0.1943 - loss/label_dissimilarity: 0.4658 - metric/dice_binary: 0.6500 - loss/image_dissimilarity: -0.3276 - loss/weighted_image_dissimilarity: -0.0328 - metric/tre: 20.3225 - loss/weighted_label_dissimilarity: 0.4658 - loss: 0.4331 - loss/regularization: 1.6555e-07 - val_metric/foreground_pred: 0.1277 - val_loss/weighted_regularization: 5.8227e-10 - val_metric/dice_float: 0.9042 - val_metric/foreground_label: 0.1419 - val_loss/label_dissimilarity: 0.3304 - val_metric/dice_binary: 0.9043 - val_loss/image_dissimilarity: -0.4436 - val_loss/weighted_image_dissimilarity: -0.0444 - val_metric/tre: 2.4321 - val_loss/weighted_label_dissimilarity: 0.3304 - val_loss: 0.2860 - val_loss/regularization: 1.1645e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM8y9Pr7EuGu",
        "colab_type": "text"
      },
      "source": [
        "The code block below downloads a pre-trained model and uses the weights from that. Please only either run the training or the pre-trained model download. If both code blocks are run, the trained model logs will be overwritten by the pre-trained model logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws2JcKNO-3lw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dd9cecda-b903-487e-c444-ee734c177ad1"
      },
      "source": [
        "! git clone https://github.com/DeepRegNet/deepreg-model-zoo.git logs/paired_ct_lung_demo_logs\n",
        "\n",
        "import zipfile\n",
        "\n",
        "fname = 'logs/paired_ct_lung_demo_logs/paired_ct_lung_demo_logs.zip'\n",
        "\n",
        "with zipfile.ZipFile(fname, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(r'logs/paired_ct_lung_demo_logs')\n",
        "\n",
        "print(\"Files unzipped!\")\n",
        "\n",
        "if os.path.exists(r'logs/paired_ct_lung_demo_logs/learn2reg_t2_paired_train_logs') is not True:\n",
        "  os.mkdir(r'logs/paired_ct_lung_demo_logs/learn2reg_t2_paired_train_logs')\n",
        "\n",
        "! cp -rf  logs/paired_ct_lung_demo_logs/learn2reg_t2_paired_train_logs logs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'logs/paired_ct_lung_demo_logs' already exists and is not an empty directory.\n",
            "Files unzipped!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FgZ3-p9RMiI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed07ef85-0de2-4743-8fc6-1bc1fc76040a"
      },
      "source": [
        "import os\n",
        "\n",
        "print(os.path.exists(r'logs/learn2reg_t2_paired_train_logs'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXAo-8e5QsVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ad89be3a-445c-4789-e622-b266882a616b"
      },
      "source": [
        "from deepreg.predict import predict\n",
        "\n",
        "######## PREDICTION ########\n",
        "\n",
        "\n",
        "log_dir = \"learn2reg_t2_paired_train_logs\"\n",
        "ckpt_path = os.path.join(\"logs\", log_dir, \"save\", \"weights-epoch2.ckpt\")\n",
        "config_path = \"logs/learn2reg_t2_paired_train_logs/config.yaml\"\n",
        "\n",
        "gpu = \"\"\n",
        "gpu_allow_growth = False\n",
        "predict(\n",
        "    gpu=gpu,\n",
        "    gpu_allow_growth=gpu_allow_growth,\n",
        "    config_path=config_path,\n",
        "    ckpt_path=ckpt_path,\n",
        "    mode=\"test\",\n",
        "    batch_size=1,\n",
        "    log_dir=log_dir,\n",
        "    sample_label=\"all\",\n",
        ")\n",
        "\n",
        "# the numerical metrics are saved in the logs directory specified"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:TODO sample_label is not used in predict\n",
            "WARNING:root:Log directory logs/learn2reg_t2_paired_train_logs exists already.\n",
            "WARNING:root:Using customized configuration.The code might break if the config of the model doesn't match the saved model.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHv3Pidn5XKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "20da544d-58cd-4297-93a2-9692db2fbedd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "######## VISUALISATION ########\n",
        "\n",
        "# Now lets load in a few samples from the predicitons and plot them\n",
        "\n",
        "# change the following line to the path to image0 label0\n",
        "path_to_image0_label0 = r\"logs/learn2reg_t2_paired_train_logs/test/label0\"\n",
        "\n",
        "# change image names if different images need to be plotted instead\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "label144 = plt.imread(os.path.join(path_to_image0_label0, \"depth144_fixed_label.png\"))\n",
        "plt.imshow(label144)\n",
        "plt.title(\"Label\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "pred144 = plt.imread(os.path.join(path_to_image0_label0,\"depth144_fixed_label_pred.png\"))\n",
        "plt.imshow(pred144)\n",
        "plt.title(\"Prediction\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "label145 = plt.imread(os.path.join(path_to_image0_label0,\"depth145_fixed_label.png\"))\n",
        "plt.imshow(label145)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "pred145 = plt.imread(os.path.join(path_to_image0_label0,\"depth145_fixed_label_pred.png\"))\n",
        "plt.imshow(pred145)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "label184 = plt.imread(os.path.join(path_to_image0_label0,\"depth184_fixed_label.png\"))\n",
        "plt.imshow(label184)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 6)\n",
        "pred184 = plt.imread(os.path.join(path_to_image0_label0,\"depth184_fixed_label_pred.png\"))\n",
        "plt.imshow(pred184)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# this is the path where you want to save the visualisation as a png\n",
        "plt.plot()\n",
        "\n",
        "print(\"Visual representation of predictions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visual representation of predictions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5zUaIpPpn7N",
        "colab_type": "text"
      },
      "source": [
        "# References <a name=\"references\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3048pG3r01o",
        "colab_type": "text"
      },
      "source": [
        "[1] G. Haskins, U. Kruger, and P. Yan, “Deep learning in medical image registration: a survey,” Mach. Vis. Appl., vol. 31, no. 1, pp. 1–18, Jan. 2020.\n",
        "\n",
        "[2] Y. Hu et al., “MR to ultrasound registration for image-guided prostate interventions,” Med. Image Anal., vol. 16, no. 3, pp. 687–703, Apr. 2012.\n",
        "\n",
        "[3] J. Ramalhinho et al., “A pre-operative planning framework for global registration of laparoscopic ultrasound to CT images,” Int. J. Comput. Assist. Radiol. Surg., vol. 13, no. 8, pp. 1177–1186, Aug. 2018.\n",
        "\n",
        "[4]  M. Lorenzo-Valdés, G. I. Sanchez-Ortiz, R. Mohiaddin, and D. Rueckert, “Atlas-based segmentation and tracking of 3D cardiac MR images using non-rigid registration,” in Lecture Notes in Computer Science, 2002, vol. 2488, pp. 642–650.\n",
        "\n",
        "[5] G. Cazoulat, D. Owen, M. M. Matuszak, J. M. Balter, and K. K. Brock, “Biomechanical deformable image registration of longitudinal lung CT images using vessel information,” Phys. Med. Biol., vol. 61, no. 13, pp. 4826–4839, 2016.\n",
        "\n",
        "[6] Y. Hu, et al., “Population-based prediction of subject-specific prostate deformation for MR-to-ultrasound image registration,” Med. Image Anal., vol. 26, no. 1, pp. 332–344, Dec. 2015.\n",
        "\n",
        "[7] P. J. Besl and N. D. McKay, “Method for registration of 3-D shapes,” in Sensor Fusion IV: Control Paradigms and Data Structures, 1992, vol. 1611, pp. 586–606.\n",
        "\n",
        "[8] A. Myronenko and X. Song, “Point set registration: Coherent point drifts,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 12, pp. 2262–2275, 2010.\n",
        "\n",
        "[9] J. Ashburner, “A fast diffeomorphic image registration algorithm,” Neuroimage, vol. 38, no. 1, pp. 95–113, Oct. 2007.\n",
        "\n",
        "[10] Y. Hu, E. Gibson, D. C. Barratt, M. Emberton, J. A. Noble, and T. Vercauteren, “Conditional Segmentation in Lieu of Image Registration,” in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2019, vol. 11765 LNCS, pp. 401–409.\n",
        "\n",
        "[11] D. L. G. Hill, P. G. Batchelor, M. Holden, and D. J. Hawkes, “Medical image registration,” Phys. Med. Biol., vol. 46, no. 3, pp. R1–R45, Mar. 2001.\n",
        "\n",
        "[12] Vallières, M. et al. Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer. Sci Rep 7, 10117 (2017). doi: 10.1038/s41598-017-10371-5\n",
        "\n",
        "[13] Litjens, et al., 2014. Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge. Medical image analysis, 18(2), pp.359-373.\n",
        "\n",
        "[14] Hering, A, , Murphy,K., and van Ginneken, B. (2020). Lean2Reg Challenge: CT Lung Registration - Training Data [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3835682\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeJaUrqV3dx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFXdxNzu3duE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
