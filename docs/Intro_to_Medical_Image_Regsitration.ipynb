{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shared Intro_to_Medical_Image_Regsitration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6HaLO5rCf9",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Medical Image Registration using Deep Neural Networks with DeepReg\n",
        "\n",
        "### Nina Montana Brown, Yunguan Fu, Shaheer U. Saeed, Zac Baum, Adria Casamitjana, Alex Grimwood, Ester Bonmati, Tom Vercauteeren, Matt Clarkson, Yipeng Hu\n",
        "\n",
        "### Who am I missing? Affiliations?\n",
        "\n",
        "# Table of Contents - Update\n",
        "1. [Set-up](#setup)\n",
        "2. [Introduction to Registration](#IntroReg)\n",
        "3. [Registration with Deep Learning](#DeepRegistrationIntro)\n",
        "4. [Two classical registration examples](#classical-examples) \n",
        "5. [Medical Image registration using an adapted DeepReg Demo](#deep-example)\n",
        "6. [References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7RjsIyi7CqZ",
        "colab_type": "text"
      },
      "source": [
        "# Objective\n",
        "This tutorial introduces a new open-source project [DeepReg](https://github.com/DeepRegNet/DeepReg), which is designed to be a rookie-friendly package for researchers interested in image registration using deep learning. \n",
        "\n",
        "A previous MICCAI workshop [learn2reg](https://learn2reg.github.io/) provided an excelent example of novel algorithms and interesting approaches in this active research area, whilst this tutorial explores the strenghth of the simple, yet generalisable deisgn of DeepReg. \n",
        "- Explain basic concepts in medical image registration;\n",
        "- Explore the links between the modern algorithms using neural networks and the classical iterative algorithms (which can also be readily implemented using DeepReg!);\n",
        "- Perhaps more importantly, introduce the versatile capability of DeepReg, with diverse examples in real clinical challenges.\n",
        "\n",
        "All of this will require minimum scripting and coding expereince with DeepReg, accomanying a set of well-written documentation and a growing number of demos using real, open-accesible clinical data. This tutorial will get you started with a number of examples by step-by-step instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey3CnrZuBtCi",
        "colab_type": "text"
      },
      "source": [
        "# Set-up <a name=\"setup\"></a>\n",
        "This tutorial depends on the package [DeepReg](https://github.com/DeepRegNet/DeepReg), which in turn has external dependencies which are managed by `pip`. \n",
        "\n",
        "To ensure the demo'd algorithms can run from this Google Colab notebook, you will need to mount your Google Drive and clone the repository into it. You can do this by running the following commands.\n",
        "\n",
        "You can also follow along in a local copy of the repo - in this case, follow instructions for set up at [the quickstart guide](https://github.com/DeepRegNet/DeepReg/blob/master/docs/quick_start.md).\n",
        "\n",
        "Training DNNs is computationally expensive. We have tested this demo with GPUs provided by Google through Google Colab. Training times have been roughly measured and indicated where appropriate. You can run this on CPU but we have not tested how long it would take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBep99T7DMvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "5af99e4c-5e89-4c6b-8d75-ce05dedc4d07"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "! pwd\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8494d2ee6737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pwd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    240\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLZnPHlqZM9R",
        "colab_type": "text"
      },
      "source": [
        "Once the drive is mounted, ensure that you have GPU enabled for more efficient training.\n",
        "\n",
        "To do this, go to the Edit tab on the upper left hand bar: Edit > Click on Notebook Settings > Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oedTHyNoDWzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ee9383b-005f-4a71-f9c1-418f53942488"
      },
      "source": [
        "import os\n",
        "# Make a directory in your google drive namde \"MICCAI_2020_reg_tutorial\"\n",
        "if not os.path.exists(\"./MICCAI_2020_reg_tutorial\"):\n",
        "  os.makedirs(\"./MICCAI_2020_reg_tutorial\")\n",
        "# Move into the dir\n",
        "%cd ./MICCAI_2020_reg_tutorial\n",
        "! pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MICCAI_2020_reg_tutorial\n",
            "/content/MICCAI_2020_reg_tutorial\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCMVBQEKnrNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Clone the DeepReg repository which contains the code\n",
        "! git clone https://github.com/DeepRegNet/DeepReg.git\n",
        "%cd ./DeepReg/\n",
        "# pip install into the notebook env\n",
        "! pip install -e .\n",
        "# ! pip install wget\n",
        "! pip install pyyaml==5.1\n",
        "! pwd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn1IiKXOrpmJ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Registration <a name=\"IntroReg\"></a>\n",
        "\n",
        "Image registration is an essential process in many clinical applications and computer assisted interventions **cite**. \n",
        "\n",
        "Applications of medical image registration (MIR) include - but are not limited to:\n",
        "* Longitudinal comparison of images for a given patient with the same imaging modality: for example, comparing the outcome of given cancer treatment in a patients' CT scans over time. **cite**\n",
        "* Inter-subject comparison: for example, population study using neuroimaging. **cite**\n",
        "* Multi-modal registration for image guided surgery: for example, aligning real-time ultrasound scans to pre-operative CT or MRI scans to real-time achieve guidance in neurosurgical or abdominal applications. **cite**\n",
        "* Atlas-based image segmentation: aligning new images to those carefully segmented, such that the reference segmentations can be propagated to those new images.\n",
        "\n",
        "Image registration is the mapping of one image coordinate system to another, and can be sub-divided into rigid registrations and non-rigid registrations, depending on whether or not higher-dimesional tissue deformations is modelled as oppose to, for exmaple, a 6 degree-of-freedom (3 translational axes + 3 rotational axes) srigid transformation. Data may be aligned in many ways - spatially or temporally being two key ones.\n",
        "\n",
        "Typically, we refer to one of the images in the pair as the *moving* image and the other as the *fixed* image. The goal is to find the *correspondence* that aligns the moving image to the fixed image - the transform will project the *moving* coordinates into the *fixed* coordinate space. The correspondence specifies the mapping between all voxels from one image to those from another image. The correspondence can be represented by a dense displacment field (DDF), defined as a set of displacement vectors for all pixels or voxels from one image to another. By using these displacement vectors, one image can be \"warped\" to become more \"similar\" to another.\n",
        "\n",
        "\n",
        "## Classical Registration Methods\n",
        "Image registration has been an active area of research for decades. Historically, the so-called classical algorithms pose registration as an optimisation problem between a given pair of moving and fixed images. In these methods, a pre-defined transformation model, rigid or nonrigid, is iteratively optimised to minimise a similarty measure - a metric that quantifies how \"similar\" between the warped moving image and the fixed image. \n",
        "\n",
        "The similarity measure can be a function sampling only important image features (extracted from a pre-processing step) or directly sampling all intensity values from both images.\n",
        "\n",
        "* **Feature-based registration**: for example, a transformation between point clouds - a type of features widely used in many applications - can be estimated using Iterative Closest Point (ICP) or coherent point drift (CPD), for rigid or nonrigid transformation, respectively. The basis of ICP is to iteratively minimise the distance between the two point clouds by matching the points from one set to the closest point in the other set. This is done by searching for the minimum Euclidean distance for each point to the other cloud(otherwise known as the L2 distance), simply, $$L_2 = \\sqrt(\\vec{x_1} - \\vec{x_2})^2$$ The transformation can be estimated from the found set of pairs, or correspondence, applying the transformation, and repeating the process many times.\n",
        "\n",
        "  Another type of features are additional labels to highlight specific regions of interest (ROIs) to the clinician in the scan(s) - *segmentations*. These are typically provided as binary masks of organs that match the image dimensions, and refer to specific images or sets of images. While these data can be used to compute other types of similarity measures (such as overlap measure Dice) to drive a registration algorithm, they may be considered as weak labels to drive a learning-based registration algotithms - therefore we refere these segmentations as labels in this tutorial - more on this later. ;)\n",
        "\n",
        "* **Intensity-based registration**: Typically, medical imaging data does not come in point cloud format, but rather, 2D, 3D and 4D matrices with a range of intensity values at each pixel or voxel. As such, different measures can be used directly on the intensity distributions of the data to measure similarity between the data. Examples of measures are cross correlation, mutual information and simple sum-square-difference - these intensity-based algorithms can optimise a transformation model directly using images without the feature extraction step.\n",
        "\n",
        "Many today's deep-learning-based methods have been heavily influnced and derived from these prior work. (You can cite Dave's book here)\n",
        "\n",
        "\n",
        "## Why use Deep Learning for MIR?\n",
        "Usually, classical methods are unable to handle real-time registration of large point sets or feature intensity distributions owing to their computationally intense nature, especially in the case of 3D nonrigid registration. State-of-the-art classical methods are implemented on GPU still struggle for real-time performance for many time-critical clinical applications.\n",
        "\n",
        "Secondly, classical algorithms are inherently pairwise approaches that can not take into account population data statistics directly and relying on well-designed transformation models and valid similarity being avaialable and robust, challenging for many real-world tasks.\n",
        "\n",
        "In contrast, the computationally efficient inference and the ability to model complex, non-linear transformations of learning-based methods has motivated the development of neural networks which infer the optimal transformation from unseen data **cite**. \n",
        "\n",
        "However, it is important to point out that \n",
        "* Many deep-learning-based methods are still subject to these limitations, especially those that borrow transformation models and similarity measures directly from the classical methods\n",
        "* Deep learning models are limited at inference time by how the model was trained - it is well known that deep learning models can overfit to the training data.\n",
        "* Classical algorithms have been refined for many clinical applications and still work really well.\n",
        "\n",
        "# Image Registration with Deep Learning <a name=\"DeepRegistrationIntro\"></a>\n",
        "\n",
        "In recent years, image registration has been re-formulated as a learning problem, in which, pairs of moving and fixed images are passed to a machine learning model (usually a neural network nowadays) to predict a transformation between the images.\n",
        "\n",
        "In this tutorial, we investigate three factors that determine a deep learning approach for image registration:\n",
        "\n",
        "1. What type of transformation is one trying to predict?\n",
        "2. What type image data are being registered? Are there any other data, such as segmentations, to support the registration?\n",
        "3. Are the data paired? Are they labeled?\n",
        "\n",
        "\n",
        "## Types of transformations\n",
        "\n",
        "We need to choose what type of transformation we want to predict. \n",
        "\n",
        "- **Predicting a dense displacement field**\n",
        "\n",
        "  Given a pair of moving and fixed images, a registration network\n",
        "  can be trained to output dense displacement field (DDF) of the same shape as the moving image. Each value in the DDF\n",
        "  can be considered as the placement of the corresponding pixel / voxel of the moving\n",
        "  image. Therefore, the DDF defines a mapping from the moving image's coordinates to the\n",
        "  fixed image.\n",
        "\n",
        "  In this tutorial, we mainly focus on DDF-based methods.\n",
        "\n",
        "- **Predict a dense velocity field**\n",
        "\n",
        "  Another option is to predict a dense velocity field (DVF) between a pair of images, such that a diffeomorphic\n",
        "  DDF can be numerically integrated. Read\n",
        "  [\"A fast diffeomorphic image registration algorithm\"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.1033&rep=rep1&type=pdf)\n",
        "  for more details.\n",
        "\n",
        "- **Predict an affine transformation**\n",
        "\n",
        "  A more constrained option is to predict an affine transformation, parameterised the\n",
        "  affine transformation matrix of 12 degrees of freedom. The DDF can then be computed to\n",
        "  resample the moving images in fixed image space.\n",
        "\n",
        "- **Predict a region of interest**\n",
        "\n",
        "  Instead of outputting the transformation between coordinates, given moving image,\n",
        "  fixed image, and a region of interest (ROI) in the moving image, the network can\n",
        "  predict the ROI in fixed image directly. Interested readers are referred to the MICCAI\n",
        "  2019 paper:\n",
        "  [Conditional segmentation in lieu of image registration](https://arxiv.org/abs/1907.00438).\n",
        "\n",
        "\n",
        "## Data availability, level of supervision and network training strategies\n",
        "\n",
        "Depending on the availability of the data labels, registration networks can be trained\n",
        "with different approaches. These will influence our loss choice.\n",
        "\n",
        "### Unsupervised\n",
        "\n",
        "When the data label is unavailable, training can be achieved via an unsupervised loss.\n",
        "The\n",
        "following is an illustration of an unsupervised DDF-based registration network.\n",
        "\n",
        "![Unsupervised DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-unsupervised.svg)\n",
        "\n",
        "The loss function often consists of the intensity based loss and deformation loss.\n",
        "\n",
        "### Weakly-supervised\n",
        "\n",
        "When there is no intensity based loss that is appropriate for the image pair one would\n",
        "like to register, the training can take a pair of corresponding moving and fixed labels\n",
        "(in addition to the image pair), represented by binary masks, to compute a label\n",
        "dissimilarity (feature based loss) to drive the registration.\n",
        "\n",
        "Combined with the regularisation on the predicted displacement field, this forms a\n",
        "weakly-supervised training. An illustration of an weakly-supervised DDF-based\n",
        "registration network is provided below.\n",
        "\n",
        "When multiple labels are available for each image, the labels can be sampled during\n",
        "training iteration, such that only one label per image is used in each iteration of the\n",
        "data set (epoch). Read [data sampling API](tutorial_sampling.md) for more details.\n",
        "\n",
        "![Weakly-supervised DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-weakly-supervised.svg)\n",
        "\n",
        "### Combined\n",
        "\n",
        "When the data label is available, combining intensity based, feature based, and\n",
        "deformation based losses together has shown superior registration accuracy, compared to\n",
        "unsupervised and weakly supervised methods. Following is an illustration of a combined\n",
        "DDF-based registration network.\n",
        "\n",
        "![Combined DDF-based registration network](https://deepregnet.github.io/DeepReg/asset/registration-ddf-nn-combined.svg)\n",
        "\n",
        "## Loss functions\n",
        "\n",
        "We aim to train a network to predict some transformation between a pair of images that is likely. To do this, we need to define what is a \"likely\" transformation. This is done via a *loss function*.\n",
        "\n",
        "The loss function defined to train a registration network will depend on the type of data we have access to.\n",
        "\n",
        "- **Intensity based (image based) loss**\n",
        "\n",
        "  This type of loss measures the dissimilarity of the fixed image and warped moving\n",
        "  image, which is adapted from the classical image registration methods. Intensity based\n",
        "  loss is modality-independent and similar to many other well-studied computer vision\n",
        "  and medical imaging tasks, such as image segmentation.\n",
        "\n",
        "  The common loss functions are normalized cross correlation (NCC), sum of squared\n",
        "  distance (SSD), and normalized mutual information (MI).\n",
        "\n",
        "- **Feature based (label based) loss**\n",
        "\n",
        "  Provided labels for the input images, a feature based loss may be used to measure the (dis)similarity of warped regions of interest. Having computed a transformation between images using the net, one of the labels is warped and compared to the ground truth image label.\n",
        " Labels are typically manually contoured organs.\n",
        "\n",
        "  The common loss function is Dice loss, Jacard and average cross-entropy over all\n",
        "  voxels, which are measures of the overlap of the ROIs. The loss will minimise the negative overlap measure (eg. l = 1 - dice_score) to maximise overlap of the regions during training.\n",
        "\n",
        "- **Deformation loss**\n",
        "\n",
        "  Additionally, training may be regularised by computing the \"likelihood\" of a given displacement field. High deformation losses point to very unlikely displacement due to high gradients of the field - typically, deformation losses ensure smoothness in the displacement field. For DDFs, typical regularisation losses are bending energy losses, L1 or L2 norms of the displacement gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRbUdZjq0_A",
        "colab_type": "text"
      },
      "source": [
        "## Image Registration with Deep Learning: Summary <a name=\"DeepRegistrationIntro\"></a>\n",
        "\n",
        "For deep learning methods, pairs of images, denoted as moving\n",
        "and fixed images, are passed to the network to predict a transformation between the images.\n",
        "\n",
        "The deep learning approach for MIR will depend on mainly three factors:\n",
        "\n",
        "1. What type of transformation is one trying to predict?\n",
        "2. What type image data are being registered? Are there any other data, such as segmentations, to support the registration?\n",
        "3. Are the data paired? Are they labeled?\n",
        "\n",
        "From this, we can design an appropriate architecture and choose an adequate loss function to motivate training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC-lMrB4Ct_",
        "colab_type": "text"
      },
      "source": [
        "# Classical Registration Examples <a name=\"classical-examples\"></a>\n",
        "\n",
        "We will define \"single-layer\" examples using functions from DeepReg, and train it to register two images.\n",
        "\n",
        "First, we will illustrate the possibility of \"self-registering\" an image to it's affine-transformed counterpart, using head and neck CT scans data.\n",
        "\n",
        "Then, we will nonrigid-register inter-subject scans, using MR images from two prostate cancer patients.\n",
        "\n",
        "## Learning an affine transformation: a \"self-registration\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNdefjurAmQ",
        "colab_type": "code",
        "tags": [],
        "colab": {}
      },
      "source": [
        "# We import some utility modules.\n",
        "import nibabel\n",
        "import tensorflow as tf \n",
        "import deepreg.model.layer as layer\n",
        "import deepreg.model.loss.image as image_loss\n",
        "import deepreg.model.loss.deform as deform_loss\n",
        "import deepreg.model.layer_util as layer_util\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# We set the plot size to some parameters.\n",
        "plt.rcParams[\"figure.figsize\"] = (100,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAh-31ZOO5Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define some utility functions first\n",
        "## optimisation\n",
        "@tf.function\n",
        "def train_step_CT(grid, weights, optimizer, mov, fix):\n",
        "    \"\"\"\n",
        "    Train step function for backprop using gradient tape\n",
        "    :param grid: reference grid return from util.get_reference_grid\n",
        "    :param weights: trainable affine parameters [1, 4, 3]\n",
        "    :param optimizer: tf.optimizers\n",
        "    :param mov: moving image [1, m_dim1, m_dim2, m_dim3]\n",
        "    :param fix: fixed image [1, f_dim1, f_dim2, f_dim3]\n",
        "    :return loss: image dissimilarity to minimise\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = layer_util.resample(vol=mov, loc=layer_util.warp_grid(grid, weights))\n",
        "        loss = image_loss.dissimilarity_fn(\n",
        "            y_true=fix, y_pred=pred, name=image_loss_name\n",
        "        )\n",
        "    gradients = tape.gradient(loss, [weights])\n",
        "    optimizer.apply_gradients(zip(gradients, [weights]))\n",
        "    return loss\n",
        "\n",
        "def plot_results(moving_image, fixed_image, warped_moving_image, nIdx):\n",
        "  \"\"\"\n",
        "  Plotting the results from training\n",
        "  :param moving_image: [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param fixed_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param warped_moving_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param nIdx: number of indices to display\n",
        "  \"\"\"\n",
        "  # Display\n",
        "  plt.figure()\n",
        "  for idx in range(nIdx):\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 1)\n",
        "      axs.imshow(moving_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 2)\n",
        "      axs.imshow(fixed_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "      axs = plt.subplot(nIdx, 3, 3 * idx + 3)\n",
        "      axs.imshow(warped_moving_image[0, ..., idx_slices[idx]], cmap=\"gray\")\n",
        "      axs.axis(\"off\")\n",
        "  plt.ion()\n",
        "  plt.suptitle('Moving Image - Fixed Image - Warped Moving Image', fontsize=200)\n",
        "  plt.show()\n",
        "\n",
        "def display(moving_image, fixed_image):\n",
        "  \"\"\"\n",
        "  Displaying our two image tensors to register\n",
        "  :param moving_image: [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  :param fixed_image:  [IM_SIZE_0, IM_SIZE_1, 3]\n",
        "  \"\"\"\n",
        "  # Display\n",
        "  idx_slices = [int(5+x*5) for x in range(int(fixed_image_size[3]/5)-1)]\n",
        "  nIdx = len(idx_slices)\n",
        "  plt.figure()\n",
        "  for idx in range(len(idx_slices)):\n",
        "      axs = plt.subplot(nIdx, 2, 2*idx+1)\n",
        "      axs.imshow(moving_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "      axs.axis('off')\n",
        "      axs = plt.subplot(nIdx, 2, 2*idx+2)\n",
        "      axs.imshow(fixed_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "      axs.axis('off')\n",
        "  plt.suptitle('Moving Image - Fixed Image', fontsize=200)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN-Lal7TMzZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We download the data first\n",
        "MAIN_PATH = os.getcwd()\n",
        "PROJECT_DIR = os.path.join(MAIN_PATH, \"demos/classical_ct_headandneck_affine\")\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "FILE_PATH = os.path.abspath(os.path.join(PROJECT_DIR, DATA_PATH, \"demo.h5\"))\n",
        "ORIGIN = \"https://github.com/YipengHu/example-data/raw/master/hnct/demo.h5\"\n",
        "\n",
        "if os.path.exists(FILE_PATH):\n",
        "    os.remove(FILE_PATH)\n",
        "else:\n",
        "  os.makedirs(os.path.join(PROJECT_DIR, DATA_PATH))\n",
        "get_file(FILE_PATH, ORIGIN)\n",
        "print(\"CT head-and-neck data downloaded: %s.\" % FILE_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlvkyqKjNrRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## registration parameters\n",
        "image_loss_name = \"ssd\"\n",
        "learning_rate = 0.01\n",
        "total_iter = int(1000)\n",
        "\n",
        "\n",
        "# Opening the file\n",
        "fid = h5py.File(FILE_PATH, \"r\")\n",
        "fixed_image = tf.cast(tf.expand_dims(fid[\"image\"], axis=0), dtype=tf.float32)\n",
        "\n",
        "# normalisation to [0,1]\n",
        "fixed_image = (fixed_image - tf.reduce_min(fixed_image)) / (\n",
        "    tf.reduce_max(fixed_image) - tf.reduce_min(fixed_image)\n",
        ")  \n",
        "\n",
        "# generate a radomly-affine-transformed moving image using DeepReg utils\n",
        "fixed_image_size = fixed_image.shape\n",
        "transform_random = layer_util.random_transform_generator(batch_size=1, scale=0.2)\n",
        "\n",
        "grid_ref = layer_util.get_reference_grid(grid_size=fixed_image_size[1:4])\n",
        "\n",
        "grid_random = layer_util.warp_grid(grid_ref, transform_random)\n",
        "moving_image = layer_util.resample(vol=fixed_image, loc=grid_random)\n",
        "\n",
        "# warp the labels to get ground-truth using the same random affine, for validation\n",
        "fixed_labels = tf.cast(tf.expand_dims(fid[\"label\"], axis=0), dtype=tf.float32)\n",
        "moving_labels = tf.stack(\n",
        "    [\n",
        "        layer_util.resample(vol=fixed_labels[..., idx], loc=grid_random)\n",
        "        for idx in range(fixed_labels.shape[4])\n",
        "    ],\n",
        "    axis=4,\n",
        ")\n",
        "\n",
        "\n",
        "# affine transformation as trainable weights\n",
        "var_affine = tf.Variable(\n",
        "    initial_value=[\n",
        "        [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]\n",
        "    ],\n",
        "    trainable=True,\n",
        ")\n",
        "\n",
        "# Optimising\n",
        "optimiser = tf.optimizers.Adam(learning_rate)\n",
        "for step in range(total_iter):\n",
        "    loss_opt = train_step_CT(grid_ref, var_affine, optimiser, moving_image, fixed_image)\n",
        "    if (step % 50) == 0:  # print info\n",
        "        tf.print(\"Step\", step, image_loss_name, loss_opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzp0FEz1Qg9Q",
        "colab_type": "text"
      },
      "source": [
        "Once the optimisation converges, we can use the optimised affine transformation to warp the moving images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc8vc_rCPdSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## warp the moving image using the optimised affine transformation\n",
        "grid_opt = layer_util.warp_grid(grid_ref, var_affine)\n",
        "warped_moving_image = layer_util.resample(vol=moving_image, loc=grid_opt)\n",
        "\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "# display\n",
        "plot_results(moving_image, fixed_image, warped_moving_image, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_1SRBlSW5PT",
        "colab_type": "text"
      },
      "source": [
        "We can see especially from the first two slices how the data has registered to the fixed image. Let's see how the transformation appears on the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17lokrA4QV-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check how the labels have been registered\n",
        "warped_moving_labels  = layer_util.resample(vol=moving_labels, loc=grid_opt)\n",
        "\n",
        "# display\n",
        "for idx_label in range(fixed_labels.shape[4]):\n",
        "    plt.figure()\n",
        "    for idx in range(len(idx_slices)):\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 1)\n",
        "        axs.imshow(moving_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\")\n",
        "        axs.axis(\"off\")\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 2)\n",
        "        axs.imshow(fixed_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\")\n",
        "        axs.axis(\"off\")\n",
        "        axs = plt.subplot(nIdx, 3, 3 * idx + 3)\n",
        "        axs.imshow(\n",
        "            warped_moving_labels[0, ..., idx_slices[idx], idx_label], cmap=\"gray\"\n",
        "        )\n",
        "        axs.axis(\"off\")\n",
        "    plt.ion()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9098lsvHXJ_E",
        "colab_type": "text"
      },
      "source": [
        "Here we can see how in some instances there is a label in the warped moving image versus the original moving image. This is an instance of conditional segmentation.\n",
        "\n",
        "## Learning a nonrigid transformation: an inter-subject registration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TBdx9dDt41n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some utility functions\n",
        "@tf.function\n",
        "def train_step(warper, weights, optimizer, mov, fix):\n",
        "    \"\"\"\n",
        "    Train step function for backpropagation using gradient tape\n",
        "\n",
        "    :param warper: warping function returned from layer.Warping\n",
        "    :param weights: trainable ddf [1, f_dim1, f_dim2, f_dim3, 3]\n",
        "    :param optimizer: tf.optimizers\n",
        "    :param mov: moving image [1, m_dim1, m_dim2, m_dim3]\n",
        "    :param fix: fixed image [1, f_dim1, f_dim2, f_dim3]\n",
        "    :return:\n",
        "        loss: overall loss to optimise\n",
        "        loss_image: image dissimilarity\n",
        "        loss_deform: deformation regularisation\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = warper(inputs=[weights, mov])\n",
        "        # Calculating the image loss between the ground truth and prediction\n",
        "        loss_image = image_loss.dissimilarity_fn(\n",
        "            y_true=fix, y_pred=pred, name=image_loss_name\n",
        "        )\n",
        "        # We calculate the deformation loss\n",
        "        loss_deform = deform_loss.local_displacement_energy(weights, deform_loss_name)\n",
        "        # Total loss is weighted\n",
        "        loss = loss_image + weight_deform_loss * loss_deform\n",
        "    # We calculate the gradients by backpropagating the loss to the trainable layer,\n",
        "    # which for registration is our ddf\n",
        "    gradients = tape.gradient(loss, [weights])\n",
        "    # Using our tf optimizer, we apply the gradients\n",
        "    optimizer.apply_gradients(zip(gradients, [weights]))\n",
        "    return loss, loss_image, loss_deform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXoa6Ac_SLXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We download the data for this example.\n",
        "MAIN_PATH = os.getcwd()\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "if not os.path.exists(os.path.join(MAIN_PATH, DATA_PATH)):\n",
        "  os.makedirs(os.path.join(MAIN_PATH, DATA_PATH))\n",
        "\n",
        "FILE_PATH = os.path.abspath(os.path.join(MAIN_PATH, DATA_PATH, \"demo2.h5\"))\n",
        "ORIGIN = \"https://github.com/YipengHu/example-data/raw/master/promise12/demo2.h5\"\n",
        "\n",
        "get_file(FILE_PATH, ORIGIN)\n",
        "print(\"Prostate MR data downloaded: %s.\" % FILE_PATH)\n",
        "\n",
        "os.chdir(MAIN_PATH)\n",
        "\n",
        "DATA_PATH = \"dataset\"\n",
        "FILE_PATH = os.path.join(MAIN_PATH, DATA_PATH, \"demo2.h5\")\n",
        "\n",
        "fid = h5py.File(FILE_PATH, \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "OPKCDDULPT3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We define some registration parameters\n",
        "image_loss_name = \"lncc\" # local normalised cross correlation loss between images\n",
        "deform_loss_name = \"bending\" # Loss to measure the bending energy of the ddf\n",
        "weight_deform_loss = 10 # we weight the deformation loss\n",
        "learning_rate = 0.1\n",
        "total_iter = int(3000) # This will train for longer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNk5DbimPT3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We get our two subject images from our datasets\n",
        "moving_image = tf.cast(tf.expand_dims(fid[\"image0\"], axis=0), dtype=tf.float32)\n",
        "fixed_image = tf.cast(tf.expand_dims(fid[\"image1\"], axis=0), dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYo61smNPT30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We initialise our layers\n",
        "fixed_image_size = fixed_image.shape\n",
        "\n",
        "# Creating our DDF tensor that can be trained\n",
        "# The DDF will be of shape [IM_SIZE_1, IM_SIZE_2, 3],\n",
        "# representing the displacement field at each dimension.\n",
        "var_ddf = tf.Variable(initialiser(fixed_image_size + [3]), name=\"ddf\", trainable=True)\n",
        "\n",
        "# We create a warping layer and initialise an optimizer\n",
        "warping = layer.Warping(fixed_image_size=fixed_image_size[1:4])\n",
        "optimiser = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "## Training the layer\n",
        "## With GPU this takes about 5 minutes.\n",
        "for step in range(total_iter):\n",
        "    # Call the gradient tape function\n",
        "    loss_opt, loss_image_opt, loss_deform_opt = train_step(\n",
        "        warping, var_ddf, optimiser, moving_image, fixed_image\n",
        "    )\n",
        "    if (step % 50) == 0:  # print info at every 50th step\n",
        "        tf.print(\n",
        "            \"Step\",\n",
        "            step,\n",
        "            \"loss\",\n",
        "            loss_opt,\n",
        "            image_loss_name,\n",
        "            loss_image_opt,\n",
        "            deform_loss_name,\n",
        "            loss_deform_opt,\n",
        "        )\n",
        "        # Visualising loss during training\n",
        "        # plt.figure()\n",
        "        # fig, axs = plt.subplots(1, 3)\n",
        "        # warped_moving_image = warping(inputs=[var_ddf, moving_image])\n",
        "        # axs[0].imshow(moving_image[0, ..., 12])\n",
        "        # axs[1].imshow(fixed_image[0, ..., 12])\n",
        "        # axs[2].imshow(warped_moving_image[0, ..., 12])\n",
        "        # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCBZ1DXHPT32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Warp the moving image using the optimised ddf and the warping layer.\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "warped_moving_image = warping(inputs=[var_ddf, moving_image])\n",
        "plot_results(moving_image, fixed_image, warped_moving_image, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dArjLUqbY_JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We can observe the effects of the warping on the moving label using the optimised affine transformation\n",
        "moving_label = tf.cast(tf.expand_dims(fid[\"label0\"], axis=0), dtype=tf.float32)\n",
        "fixed_label = tf.cast(tf.expand_dims(fid[\"label1\"], axis=0), dtype=tf.float32)\n",
        "\n",
        "idx_slices = [int(5 + x * 5) for x in range(int(fixed_image_size[3] / 5) - 1)]\n",
        "nIdx = len(idx_slices)\n",
        "warped_moving_labels = warping(inputs=[var_ddf, moving_label])\n",
        "plot_results(moving_label, fixed_label, warped_moving_labels, nIdx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPAH2zDq4BjB",
        "colab_type": "text"
      },
      "source": [
        "# Medical Image registration using an adapted DeepReg Demo <a name=\"deep-example\"></a>\n",
        "\n",
        "Now, we will build a more complex demo to investigate a clinical case, using deep-learning.\n",
        "\n",
        "We will perform an inter-subject registration: aligning ct images from different patients. The images are all acquired at the similar timepoint in the breathing cycle for all patients. This kind of registration is useful for determining how one stimulus affects multiple patietns. If a drug or invasive procedure is administered to multiple patients, registering the images from different patients can give medical professsionals a sense of how each patient is responding in comparison to others. An example of such an application can be seen in **[2]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuxx6u164ABI",
        "colab_type": "text"
      },
      "source": [
        "The data files used in this tutorial have been pre-arranged in a folder, required by the DeepReg unpaired dataset loader, and can be downloaded as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6kkjw4-JdPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "92d00a5e-ce7c-4b57-e995-ed1cc2b1b22e"
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "MAIN_PATH = os.getcwd()\n",
        "PROJECT_DIR = os.path.join(MAIN_PATH, \"demos/unpaired_ct_lung/\")\n",
        "if os.path.exists(os.path.join(PROJECT_DIR,'data')):\n",
        "  shutil.rmtree(os.path.join(PROJECT_DIR,'data'))\n",
        "\n",
        "URL_ZIP = \"https://github.com/YipengHu/example-data/archive/unpaired_ct_lung.zip\"\n",
        "data_zip = get_file(os.path.abspath(os.path.join(PROJECT_DIR,'data.zip')), URL_ZIP)\n",
        "with zipfile.ZipFile(data_zip, \"r\") as zf:\n",
        "    zf.extractall(PROJECT_DIR)\n",
        "\n",
        "DEF_PATH = os.path.join(PROJECT_DIR,\"example-data-unpaired_ct_lung\")\n",
        "os.rename(DEF_PATH, os.path.join(PROJECT_DIR,'data'))\n",
        "\n",
        "if os.path.exists(data_zip):\n",
        "    os.remove(data_zip)\n",
        "\n",
        "print(\"Files downloaded and unzipped.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/YipengHu/example-data/archive/unpaired_ct_lung.zip\n",
            "140484608/Unknown - 13s 0us/stepFiles downloaded and unzipped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJmrcRhh5RpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "d5b1de7e-07d0-46a3-8b93-5b64d0c5be06"
      },
      "source": [
        "\n",
        "from deepreg.train import train\n",
        "! ls\n",
        "! pwd\n",
        "\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "######## NOW WE DO THE TRAINING ########\n",
        "\n",
        "MAIN_PATH = os.getcwd()\n",
        "\n",
        "gpu = \"0\"\n",
        "gpu_allow_growth = False\n",
        "ckpt_path = \"\"\n",
        "log_dir = \"learn2reg_t2_unpaired_train_logs\"\n",
        "\n",
        "config_path = [\n",
        "    os.path.join(MAIN_PATH, \"deepreg/config/test/ddf.yaml\"),\n",
        "     os.path.join(MAIN_PATH, \"demos/unpaired_ct_lung/unpaired_ct_lung.yaml\")\n",
        "]\n",
        "\n",
        "# We use\n",
        "train(\n",
        "    gpu=gpu,\n",
        "    config_path=config_path,\n",
        "    gpu_allow_growth=gpu_allow_growth,\n",
        "    ckpt_path=ckpt_path,\n",
        "    log_dir=log_dir,\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MICCAI_2020_reg_tutorial  sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e6dc9b69f326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mgpu_allow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_allow_growth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m )\n",
            "\u001b[0;32m/content/MICCAI_2020_reg_tutorial/DeepReg/deepreg/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(gpu, config_path, gpu_allow_growth, ckpt_path, log_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# load config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     config, log_dir = build_config(\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MICCAI_2020_reg_tutorial/DeepReg/deepreg/train.py\u001b[0m in \u001b[0;36mbuild_config\u001b[0;34m(config_path, log_dir, ckpt_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# load and backup config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mconfig_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MICCAI_2020_reg_tutorial/DeepReg/deepreg/config/parser.py\u001b[0m in \u001b[0;36mload_configs\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconfig_path_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mconfig_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/deepreg/config/test/ddf.yaml'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHv3Pidn5XKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from deepreg.predict import predict\n",
        "\n",
        "######## PREDICTION ########\n",
        "\n",
        "log_dir = \"learn2reg_t2_unpaired_train_logs\"\n",
        "ckpt_path = os.path.join(\"logs\", log_dir, \"save\", \"weights-epoch2.ckpt\")\n",
        "\n",
        "gpu = \"\"\n",
        "gpu_allow_growth = False\n",
        "predict(\n",
        "    gpu=gpu,\n",
        "    gpu_allow_growth=gpu_allow_growth,\n",
        "    ckpt_path=ckpt_path,\n",
        "    mode=\"test\",\n",
        "    batch_size=1,\n",
        "    log_dir=log_dir,\n",
        "    sample_label=\"all\",\n",
        ")\n",
        "\n",
        "# the numerical metrics are saved in the logs directory specified\n",
        "\n",
        "######## VISUALISATION ########\n",
        "\n",
        "# Now lets load in a few samples from the predicitons and plot them\n",
        "\n",
        "# change the following line to the path to image0 label0\n",
        "path_to_image0_label0 = r\"\"\n",
        "os.chdir(path_to_image0_label0)\n",
        "\n",
        "# change image names if different images need to be plotted instead\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "label144 = plt.imread(\"depth144_fixed_label.png\")\n",
        "plt.imshow(label144)\n",
        "plt.title(\"Label\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "pred144 = plt.imread(\"depth144_fixed_pred.png\")\n",
        "plt.imshow(pred144)\n",
        "plt.title(\"Prediction\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "label145 = plt.imread(\"depth145_fixed_label.png\")\n",
        "plt.imshow(label145)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "pred145 = plt.imread(\"depth145_fixed_pred.png\")\n",
        "plt.imshow(pred145)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "label184 = plt.imread(\"depth184_fixed_label.png\")\n",
        "plt.imshow(label184)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3, 2, 6)\n",
        "pred184 = plt.imread(\"depth184_fixed_pred.png\")\n",
        "plt.imshow(pred184)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# this is the path where you want to save the visualisation as a png\n",
        "path_to_save_fig = r\"\"\n",
        "plt.savefig(os.path.join(path_to_save_fig, \"labels_and_preds.png\"))\n",
        "\n",
        "print(\"Visual representation of predictions saved to path specified\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
