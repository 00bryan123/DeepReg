{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_Medical_Image_Regsitration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6HaLO5rCf9",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Medical Image Registration using Open Source Toolboxes\n",
        "\n",
        "### Authors, Acknowledgements etc etc\n",
        "\n",
        "# Table of Contents\n",
        "1. [Set-up](#setup)\n",
        "2. [Introduction to Registration](#IntroReg)\n",
        "3. [Registration with Deep Learning](#DeepRegistrationIntro)\n",
        "4. [Third Example](#third-example)\n",
        "5. [Fourth Example](#fourth-examplehttpwwwfourthexamplecom)\n",
        "6. [References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey3CnrZuBtCi",
        "colab_type": "text"
      },
      "source": [
        "# Set-up <a name=\"setup\"></a>\n",
        "This tutorial depends on the package DeepReg, which in turn has external dependencies which are managed by `pip`. \n",
        "\n",
        "To ensure the demo'd algorithms can run from this Google Colab notebook, you will need to mount your Google Drive and clone the repository into it. You can do this by running the following commands.\n",
        "\n",
        "You can also follow along in a local copy of the repo - in this case, follow instructions for set up at **Link to local setup instructions**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBep99T7DMvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1d710499-3488-4287-d5f5-51c56a840628"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "! pwd\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oedTHyNoDWzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4f5113ad-cf65-4b20-b4e9-d310690177dc"
      },
      "source": [
        "import os\n",
        "# Make a directory in your google drive namde \"MICCAI_2020_reg_tutorial\"\n",
        "if not os.path.exists(\"./MICCAI_2020_reg_tutorial\"):\n",
        "  os.makedirs(\"./MICCAI_2020_reg_tutorial\")\n",
        "# Move into the dir\n",
        "%cd ./MICCAI_2020_reg_tutorial\n",
        "! pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MICCAI_2020_reg_tutorial\n",
            "/content/MICCAI_2020_reg_tutorial\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCMVBQEKnrNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9dc2f21a-2165-45a6-d310-af39fe3ec7ff"
      },
      "source": [
        "# Clone the DeepReg repository which contains the code\n",
        "! git clone https://github.com/DeepRegNet/DeepReg.git\n",
        "%cd ./DeepReg/\n",
        "# pip install into the notebook env\n",
        "! pip install -e .\n",
        "! pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepReg'...\n",
            "remote: Enumerating objects: 275, done.\u001b[K\n",
            "remote: Counting objects: 100% (275/275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 4780 (delta 168), reused 165 (delta 76), pack-reused 4505\u001b[K\n",
            "Receiving objects: 100% (4780/4780), 10.82 MiB | 17.98 MiB/s, done.\n",
            "Resolving deltas: 100% (3142/3142), done.\n",
            "/content/MICCAI_2020_reg_tutorial/DeepReg\n",
            "Obtaining file:///content/MICCAI_2020_reg_tutorial/DeepReg\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest-dependency\n",
            "  Downloading https://files.pythonhosted.org/packages/69/6d/cfd6d654877f75e0368e4040f1cf0350dd9f427b578bf7b685af629f8167/pytest-dependency-0.5.1.tar.gz\n",
            "Collecting isort\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/b4/b008b7ec1fc3b60923277c955de488d533613f0ac4338d89b2cf353ac5ae/isort-5.1.4-py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (3.2.2)\n",
            "Collecting pytest-cov\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/13/ae3dec587b1cc07fb9f294e52ea9ad140266aea55adb9e12eade3625bd27/pytest_cov-2.10.0-py2.py3-none-any.whl\n",
            "Collecting simple-http-server\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/89/f1883d3e9ff76e2cf2ff962768c54318612c9bb7bd1e71a19cea7440c996/simple_http_server-0.1.7-py3-none-any.whl\n",
            "Collecting pytest>=4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f3/0a83558da436a081344aa6c8b85ea5b5f05071214106036ce341b7769b0b/pytest-5.4.3-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (3.0.2)\n",
            "Collecting pre-commit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/f0/84f3ac2978fa32096585a8ad3c695e7555a28e60e71064d8648f1adb2f92/pre_commit-2.6.0-py2.py3-none-any.whl (171kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 7.4MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/20/6326a9a0c6f0527612bae748c4c03df5cd69cf06dfb2cf59d85c6e165a6a/flake8-3.8.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.0MB/s \n",
            "\u001b[?25hCollecting testfixtures\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/36/ee5fce9fb4a4b48bf7e78c3ce2f5f6941dd8295cacc283ee84f59831de71/testfixtures-6.14.1-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (4.41.1)\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (2.10.0)\n",
            "Requirement already satisfied: tensorflow==2.2 in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from deepreg==0.1.5) (1.18.5)\n",
            "Collecting black\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/bb/ad34bbc93d1bea3de086d7c59e528d4a503ac8fe318bd1fa48605584c3d2/black-19.10b0-py36-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.0MB/s \n",
            "\u001b[?25hCollecting seed-isort-config\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/85/78c654684181ecab22c279d425479a5265f9c1fa144939b35bdf1ac7cc35/seed_isort_config-2.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepreg==0.1.5) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepreg==0.1.5) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepreg==0.1.5) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->deepreg==0.1.5) (2.4.7)\n",
            "Collecting coverage>=4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/19/ad8f558e885a11a68111300ee23a632cc46a5ef6aaed875ba56460076fa7/coverage-5.2-cp36-cp36m-manylinux1_x86_64.whl (229kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (0.2.5)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (1.9.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (1.7.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (8.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.6->deepreg==0.1.5) (20.4)\n",
            "Collecting nodeenv>=0.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/54/538edbe88c87f6cd71f009145a56264622de84508f6ad1ae4d76976f47be/nodeenv-1.4.0-py2.py3-none-any.whl\n",
            "Collecting importlib-resources; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Collecting toml\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
            "Collecting virtualenv>=20.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/24/27197e06ff269e54a05fcb7e270d5648fafe076eafd4ec289859a3dc4905/virtualenv-20.0.27-py2.py3-none-any.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 9.2MB/s \n",
            "\u001b[?25hCollecting cfgv>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/82/49913e721128ff16d6b7cf304f513de7bba698583b045dfb9c4b3bb2f467/cfgv-3.1.0-py2.py3-none-any.whl\n",
            "Collecting identify>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/38/a175257f534244bc4236a29ec039162d2c3e101b8671da55780eeab2f462/identify-1.4.24-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.5MB/s \n",
            "\u001b[?25hCollecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 650kB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepreg==0.1.5) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.30.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (2.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (3.12.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.12.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->deepreg==0.1.5) (0.3.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from black->deepreg==0.1.5) (2019.12.20)\n",
            "Collecting typed-ast>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 44.4MB/s \n",
            "\u001b[?25hCollecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting pathspec<1,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/d0/887c58853bd4b6ffc7aa9cdba4fc57d7b979b45888a6bd47e4568e1cf868/pathspec-0.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=6.5 in /usr/local/lib/python3.6/dist-packages (from black->deepreg==0.1.5) (7.1.2)\n",
            "Collecting aspy.refactor-imports\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/35/dd64697a0ff36a192c6b333b66bd425488384204576ae846f80093a08a8e/aspy.refactor_imports-2.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=4.6->deepreg==0.1.5) (3.1.0)\n",
            "Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from virtualenv>=20.0.8->pre-commit->deepreg==0.1.5) (3.0.12)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/0a/490fa011d699bb5a5f3a0cf57de82237f52a6db9d40f33c53b2736c9a1f9/distlib-0.3.1-py2.py3-none-any.whl (335kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (49.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (0.4.1)\n",
            "Collecting cached-property\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/86/85c1be2e8db9e13ef9a350aecd6dea292bd612fa288c2f40d035bb750ded/cached_property-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (4.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->deepreg==0.1.5) (3.1.0)\n",
            "Building wheels for collected packages: pytest-dependency\n",
            "  Building wheel for pytest-dependency (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-dependency: filename=pytest_dependency-0.5.1-cp36-none-any.whl size=8200 sha256=2896b332d68efefcd03b5b480a7198f877c6ddef8091c730ca9478399f04d35e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/32/57/2b5f4997722ef4021bc4f32b793c0798ccf734a954bd9533ab\n",
            "Successfully built pytest-dependency\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pre-commit 2.6.0 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, pytest-dependency, isort, coverage, pytest-cov, simple-http-server, nodeenv, importlib-resources, toml, distlib, appdirs, virtualenv, cfgv, identify, pre-commit, pyflakes, mccabe, pycodestyle, flake8, testfixtures, argparse, typed-ast, pathspec, black, cached-property, aspy.refactor-imports, seed-isort-config, deepreg\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "  Running setup.py develop for deepreg\n",
            "Successfully installed appdirs-1.4.4 argparse-1.4.0 aspy.refactor-imports-2.1.1 black-19.10b0 cached-property-1.5.1 cfgv-3.1.0 coverage-5.2 deepreg distlib-0.3.1 flake8-3.8.3 identify-1.4.24 importlib-resources-3.0.0 isort-5.1.4 mccabe-0.6.1 nodeenv-1.4.0 pathspec-0.8.0 pluggy-0.13.1 pre-commit-2.6.0 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-5.4.3 pytest-cov-2.10.0 pytest-dependency-0.5.1 seed-isort-config-2.2.0 simple-http-server-0.1.7 testfixtures-6.14.1 toml-0.10.1 typed-ast-1.4.1 virtualenv-20.0.27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/MICCAI_2020_reg_tutorial/DeepReg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn1IiKXOrpmJ",
        "colab_type": "text"
      },
      "source": [
        "#Introduction to Registration <a name=\"IntroReg\"></a>\n",
        "\n",
        "Image registration is an essential process in many clinical applications and computer assisted interventions **cite**. Applications of medical image registration include - but are not limited to:\n",
        "* Longitudinal comparison of images for a given patient with the same imaging modality: for example, comparing the outcome of given cancer treatment in a patients' CT scans over time. **cite**\n",
        "* Inter-subject comparison: for example, **cite**\n",
        "* Multi-modal registration for image guided surgery: for example, aligning real-time ultrasound scans to pre-operative CT or MRI scans to real-time achieve guidance in neurosurgical or abdominal applications. **cite**\n",
        "\n",
        "Image registration is the mapping of one coordinate system to another, and can be sub-divided into rigid registrations and non-rigid registrations (depending on whether or not tissue deformations is modelled).\n",
        "\n",
        "## Classical Registration Methods\n",
        "* PBR - ICP **example**\n",
        "* DDF\n",
        "\n",
        "## Why use Deep Learning for MIR?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRbUdZjq0_A",
        "colab_type": "text"
      },
      "source": [
        "# Image Registration with Deep Learning <a name=\"DeepRegistrationIntro\"></a>\n",
        "\n",
        "For deep learning methods, pairs of images, denoted as moving\n",
        "and fixed images, are passed to the network to predict a transformation between the images.\n",
        "\n",
        "The deep learning \"solution\" for MIR will depend on mainly three factors:\n",
        "1. What type of transformation is one trying to predict?\n",
        "2. What type data is being registered?\n",
        "3. Is the data paired and/or labeled?\n",
        "\n",
        "## Types of transformations\n",
        "\n",
        "- **Predicting a dense displacement field**\n",
        "\n",
        "  Given a pair of moving and fixed images, a registration network\n",
        "  can be trained to output dense displacement field (DDF) of the same shape as the moving image. Each value in the DDF\n",
        "  can be considered as the placement of the corresponding pixel / voxel of the moving\n",
        "  image. Therefore, the DDF defines a mapping from the moving image's coordinates to the\n",
        "  fixed image.\n",
        "\n",
        "  In this tutorial, we mainly focus on DDF-based methods.\n",
        "\n",
        "- **Predict a dense velocity field**\n",
        "\n",
        "  Another option is to predict a dense velocity field (DVF) between a pair of images, such that a diffeomorphic\n",
        "  DDF can be numerically integrated. Read\n",
        "  [\"A fast diffeomorphic image registration algorithm\"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.1033&rep=rep1&type=pdf)\n",
        "  for more details.\n",
        "\n",
        "- **Predict an affine transformation**\n",
        "\n",
        "  A more constrained option is to predict an affine transformation, parameterised the\n",
        "  affine transformation matrix of 12 degrees of freedom. The DDF can then be computed to\n",
        "  resample the moving images in fixed image space.\n",
        "\n",
        "- **Predict a region of interest**\n",
        "\n",
        "  Instead of outputting the transformation between coordinates, given moving image,\n",
        "  fixed image, and a region of interest (ROI) in the moving image, the network can\n",
        "  predict the ROI in fixed image directly. Interested readers are referred to the MICCAI\n",
        "  2019 paper:\n",
        "  [Conditional segmentation in lieu of image registration](https://arxiv.org/abs/1907.00438)\n",
        "\n",
        "\n",
        "## Data availability - level of supervision\n",
        "\n",
        "Depending on the availability of the data labels, registration networks can be trained\n",
        "with different approaches:\n",
        "\n",
        "### Unsupervised\n",
        "\n",
        "When the data label is unavailable, training can be achieved via an unsupervised loss.\n",
        "The\n",
        "following is an illustration of an unsupervised DDF-based registration network.\n",
        "\n",
        "![Unsupervised DDF-based registration network](asset/registration-ddf-nn-unsupervised.svg \":size=600\")\n",
        "\n",
        "The loss function often consists of the intensity based loss and deformation loss.\n",
        "\n",
        "### Weakly-supervised\n",
        "\n",
        "When there is no intensity based loss that is appropriate for the image pair one would\n",
        "like to register, the training can take a pair of corresponding moving and fixed labels\n",
        "(in addition to the image pair), represented by binary masks, to compute a label\n",
        "dissimilarity (feature based loss) to drive the registration.\n",
        "\n",
        "Combined with the regularisation on the predicted displacement field, this forms a\n",
        "weakly-supervised training. An illustration of an weakly-supervised DDF-based\n",
        "registration network is provided below.\n",
        "\n",
        "When multiple labels are available for each image, the labels can be sampled during\n",
        "training iteration, such that only one label per image is used in each iteration of the\n",
        "data set (epoch). Read [data sampling API](tutorial_sampling.md) for more details.\n",
        "\n",
        "![Weakly-supervised DDF-based registration network](asset/registration-ddf-nn-weakly-supervised.svg \":size=600\")\n",
        "\n",
        "### Combined\n",
        "\n",
        "When the data label is available, combining intensity based, feature based, and\n",
        "deformation based losses together has shown superior registration accuracy, compared to\n",
        "unsupervised and weakly supervised methods. Following is an illustration of a combined\n",
        "DDF-based registration network.\n",
        "\n",
        "![Combined DDF-based registration network](asset/registration-ddf-nn-combined.svg \":size=600\")\n",
        "\n",
        "## Losses - depend on supervision!\n",
        "\n",
        "To train the deep registration network, measures of similarity between the input images and their transformations are mainly used:\n",
        "\n",
        "- **Intensity based (image based) loss**\n",
        "\n",
        "  This type of loss measures the dissimilarity of the fixed image and warped moving\n",
        "  image, which is adapted from the classical image registration methods. Intensity based\n",
        "  loss is modality-independent and similar to many other well-studied computer vision\n",
        "  and medical imaging tasks, such as image segmentation.\n",
        "\n",
        "  The common loss functions are normalized cross correlation (NCC), sum of squared\n",
        "  distance (SSD), and normalized mutual information (MI).\n",
        "\n",
        "- **Feature based (label based) loss**\n",
        "  Provided labels for the input images, a feature based loss may be used to measure the (dis)similarity of warped regions of interest. Having computed a transformation between images using the net, one of the labels is warped and compared to the ground truth image label.\n",
        " Labels are typically manually contoured organs.\n",
        "\n",
        "  The common loss function is Dice loss, Jacard and average cross-entropy over all\n",
        "  voxels.\n",
        "\n",
        "- **Deformation loss**\n",
        "  Additionally, training may be regularised by computing the \"likelihood\" of a given displacement field. For DDFs, typical regularisation losses are bending energy losses, L1 or L2 norms of the displacement gradients.\n",
        "\n",
        "  **Examples**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DreJQcGpP2k",
        "colab_type": "text"
      },
      "source": [
        "To demonstrate the power of deep neural networks to perform registration, we provide the following examples:\n",
        "1. Comparing PBR to a simple net for a rigid deformation\n",
        "2. Comparing PBR to a simple net for a non-rigid deformation\n",
        "3. Building a DNN using DeepReg\n",
        "4. Using a DNN for MIR: adapted demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC-lMrB4Ct_",
        "colab_type": "text"
      },
      "source": [
        "# Comparing PBR to a simple net for a rigid deformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNdefjurAmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "094acfba-4d4d-41d4-f958-e5d6daff5ef0"
      },
      "source": [
        "! ls\n",
        "import nibabel\n",
        "import tensorflow as tf \n",
        "import deepreg.model.layer as layer\n",
        "import deepreg.model.loss.image as image_loss\n",
        "import deepreg.model.layer_util as layer_util\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## registration parameters\n",
        "image_loss_name = 'ssd'\n",
        "learning_rate = 0.1\n",
        "total_iter = int(1000)\n",
        "\n",
        "\n",
        "## load image\n",
        "load_image = lambda fn:tf.cast(tf.expand_dims(nibabel.load(fn).dataobj, axis=0), dtype=tf.float32)\n",
        "moving_image = load_image(os.path.join(os.getcwd(), '/data/mr_us/unpaired/train/images/case000000.nii.gz'))\n",
        "# fixed_image = load_image('./data/mr_us/unpaired/train/images/case000001.nii.gz')\n",
        "\n",
        "# random affine-transformed fixed image\n",
        "fixed_image_size = moving_image.shape\n",
        "random_transform = layer_util.random_transform_generator(batch_size=1, scale=0.2)\n",
        "grid_ref = layer_util.get_reference_grid(grid_size=fixed_image_size[1:4])\n",
        "fixed_image = layer_util.resample(vol=moving_image, loc=layer_util.warp_grid(grid_ref, random_transform))\n",
        "\n",
        "\n",
        "## optimisation\n",
        "@tf.function\n",
        "def train_step(grid, weights, optimizer, mov, fix):\n",
        "    \"\"\"\n",
        "    Train step function for backprop using gradient tape    \n",
        "\n",
        "    :param grid: reference grid return from layer_util.get_reference_grid\n",
        "    :param weights: trainable affine parameters [1, 4, 3]\n",
        "    :param optimizer: tf.optimizers\n",
        "    :param mov: moving image [1, m_dim1, m_dim2, m_dim3]\n",
        "    :param fix: fixed image [1, f_dim1, f_dim2, f_dim3]\n",
        "    :return loss: image dissimilarity to minimise\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = layer_util.resample(vol=mov, loc=layer_util.warp_grid(grid, weights))\n",
        "        loss = image_loss.similarity_fn(y_true=fix, y_pred=pred, name=image_loss_name)\n",
        "    gradients = tape.gradient(loss, [weights])\n",
        "    optimizer.apply_gradients(zip(gradients, [weights]))\n",
        "    return loss\n",
        "\n",
        "# affine transformation as trainable weights\n",
        "var_affine = tf.Variable(initial_value=[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0],[0.0,0.0,0.0]]], trainable=True)\n",
        "optimiser = tf.optimizers.Adam(learning_rate)\n",
        "for step in range(total_iter):\n",
        "    loss_opt = train_step(grid_ref, var_affine, optimiser, moving_image, fixed_image)\n",
        "    if (step % 50) == 0:  # print info\n",
        "        tf.print('Step',step, image_loss_name,loss_opt)\n",
        "\n",
        "\n",
        "## predict\n",
        "pred_fixed_image = layer_util.resample(vol=moving_image, loc=layer_util.warp_grid(grid_ref, var_affine))\n",
        "\n",
        "# display\n",
        "idx_slices = [int(5+x*5) for x in range(int(fixed_image_size[3]/5)-1)]\n",
        "nIdx = len(idx_slices)\n",
        "plt.figure()\n",
        "for idx in range(len(idx_slices)):\n",
        "    axs = plt.subplot(nIdx, 3, 3*idx+1)\n",
        "    axs.imshow(moving_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "    axs.axis('off')\n",
        "    axs = plt.subplot(nIdx, 3, 3*idx+2)\n",
        "    axs.imshow(fixed_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "    axs.axis('off')\n",
        "    axs = plt.subplot(nIdx, 3, 3*idx+3)\n",
        "    axs.imshow(pred_fixed_image[0,...,idx_slices[idx]], cmap='gray')\n",
        "    axs.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t  deepreg_logo_cornflower.svg  docs\t       pytest.ini  test\n",
            "deepreg\t\t  deepreg_logo_purple.svg      LICENSE\t       README.md\n",
            "deepreg.egg-info  demos\t\t\t       pyproject.toml  setup.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/mr_us/unpaired/train/images/case000000.nii.gz'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9620f30bc35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m## load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mload_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnibabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmoving_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/data/mr_us/unpaired/train/images/case000000.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# fixed_image = load_image('./data/mr_us/unpaired/train/images/case000001.nii.gz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-9620f30bc35d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m## load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mload_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnibabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmoving_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/data/mr_us/unpaired/train/images/case000000.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# fixed_image = load_image('./data/mr_us/unpaired/train/images/case000001.nii.gz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or no access: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstat_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImageFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: '/data/mr_us/unpaired/train/images/case000000.nii.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW-Wm0J-qZ_3",
        "colab_type": "text"
      },
      "source": [
        "# Comparing PBR to a simple net for a non-ridid deformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPAH2zDq4BjB",
        "colab_type": "text"
      },
      "source": [
        "# Medical Image registration using Deep Learning Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuxx6u164ABI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
